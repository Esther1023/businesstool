from flask import Flask, render_template, request, send_file, jsonify, redirect, url_for, session
from jinja2 import TemplateSyntaxError
try:
    from docxtpl.exceptions import TemplateError as DocxTplTemplateError
except Exception:
    DocxTplTemplateError = None
import os
import tempfile
# import pandas as pd  # å»¶è¿Ÿå¯¼å…¥ä»¥é¿å…å¯åŠ¨æ—¶è¶…æ—¶
from datetime import datetime
import logging
import threading
import time
from pathlib import Path
import re
from werkzeug.utils import secure_filename

# å…¨å±€å˜é‡ç”¨äºå»¶è¿Ÿå¯¼å…¥
pd = None

def ensure_pandas_imported():
    """ç¡®ä¿pandaså·²å¯¼å…¥"""
    global pd
    if pd is None:
        logger.info("å¼€å§‹å¯¼å…¥pandas...")
        import pandas as pandas_module
        pd = pandas_module
        logger.info("pandaså¯¼å…¥å®Œæˆ")
    return pd

# å¯¼å…¥Flaskç›¸å…³æ¨¡å—
from flask import Flask, render_template, request, redirect, url_for, session, jsonify

# è®¾ç½®æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)

# å»¶è¿Ÿå¯¼å…¥OCRç›¸å…³æ¨¡å—ï¼Œé¿å…å¯åŠ¨æ—¶å¤±è´¥
template_handler = None
ocr_service = None

# å¯¼å…¥çŠ¶æ€ç®¡ç†å™¨
try:
    from stage_manager import StageManager
    STAGE_MANAGER_AVAILABLE = True
except ImportError as e:
    logger.warning(f"çŠ¶æ€ç®¡ç†å™¨å¯¼å…¥å¤±è´¥: {str(e)}")
    STAGE_MANAGER_AVAILABLE = False

# å¯¼å…¥ä¼˜åŒ–çš„OCRæœåŠ¡
try:
    from ocr_service_optimized import OptimizedOCRService as OCRService
    OCR_SERVICE_AVAILABLE = True
    logger.info("ä¼˜åŒ–OCRæœåŠ¡å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    logger.warning(f"OCRæœåŠ¡å¯¼å…¥å¤±è´¥: {str(e)}")
    OCR_SERVICE_AVAILABLE = False

# å°è¯•å¯¼å…¥æ¨¡æ¿å¤„ç†å™¨
try:
    from template_handler import TemplateHandler
    TEMPLATE_HANDLER_AVAILABLE = True
except ImportError as e:
    logger.warning(f"æ¨¡æ¿å¤„ç†å™¨å¯¼å…¥å¤±è´¥: {str(e)}")
    TEMPLATE_HANDLER_AVAILABLE = False

# åˆ›å»ºFlaskåº”ç”¨
app = Flask(__name__)

# ç®€åŒ–é…ç½®
app.secret_key = os.environ.get('SECRET_KEY', 'dev-secret-key')
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB

# è®¾ç½®æ—¥å¿—è®°å½•
log_dir = 'logs'
if not os.path.exists(log_dir):
    os.makedirs(log_dir, exist_ok=True)

# ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ä¸åŒçš„æ—¥å¿—é…ç½®
env = os.environ.get('FLASK_ENV', 'development')
if env == 'production':
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s %(levelname)s: %(message)s'
    )
else:
    # å¼€å‘ç¯å¢ƒè¾“å‡ºåˆ°æ§åˆ¶å°ï¼Œæ–¹ä¾¿è°ƒè¯•
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s %(levelname)s: %(message)s'
    )

# åˆå§‹åŒ–OCRæœåŠ¡ï¼ˆå®¹é”™å¤„ç†ï¼‰
try:
    if OCR_SERVICE_AVAILABLE:
        ocr_service = OCRService()
        logger.info("OCRæœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
    else:
        logger.warning("OCRæœåŠ¡ä¸å¯ç”¨ï¼šå¯¼å…¥å¤±è´¥")
        ocr_service = None
except Exception as e:
    logger.warning(f"OCRæœåŠ¡åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    ocr_service = None

# æ¨¡æ¿å¤„ç†å™¨å°†åœ¨éœ€è¦æ—¶åˆå§‹åŒ–
template_handler = None
if TEMPLATE_HANDLER_AVAILABLE:
    logger.info("æ¨¡æ¿å¤„ç†å™¨ç±»å¯ç”¨")
else:
    logger.warning("æ¨¡æ¿å¤„ç†å™¨ä¸å¯ç”¨ï¼šå¯¼å…¥å¤±è´¥")

# å­˜å‚¨æœ€åå¯¼å…¥æ—¶é—´
last_import_time = None

# åˆå§‹åŒ–çŠ¶æ€ç®¡ç†å™¨
stage_manager = None
if STAGE_MANAGER_AVAILABLE:
    try:
        stage_manager = StageManager('å…­å¤§æˆ˜åŒºç®€é“äº‘å®¢æˆ·.xlsx')
        logger.info("çŠ¶æ€ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        logger.warning(f"çŠ¶æ€ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        stage_manager = None

# è‡ªåŠ¨ç›‘æ§ç›¸å…³å˜é‡
auto_monitor_enabled = False
monitor_thread = None
monitor_lock = threading.Lock()
last_monitor_check = None
monitor_results = {'recent_contracts': [], 'updated_contracts': [], 'last_check': None}
# å·²å¤„ç†æ–‡ä»¶è®°å½•ï¼ˆæ–‡ä»¶è·¯å¾„ -> å¤„ç†æ—¶é—´æˆ³ï¼‰
processed_files = {}

# é”€å”®ä»£è¡¨å§“åæ ‡å‡†åŒ–å‡½æ•°
def normalize_sales_name(sales_name):
    """
    æ ‡å‡†åŒ–é”€å”®ä»£è¡¨å§“åï¼Œè§£å†³å¤§å°å†™ä¸ä¸€è‡´é—®é¢˜
    ä¾‹å¦‚ï¼šEsther.zhu å’Œ Esther.Zhu éƒ½æ ‡å‡†åŒ–ä¸º Esther.Zhu
    """
    if not sales_name or str(sales_name).lower() == 'nan':
        return ''
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºpandasçš„NaNå€¼
    try:
        pd = ensure_pandas_imported()
        if pd.isna(sales_name):
            return ''
    except:
        # å¦‚æœpandasä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•çš„æ£€æŸ¥
        if sales_name is None or (hasattr(sales_name, '__len__') and len(str(sales_name).strip()) == 0):
            return ''
    
    name = str(sales_name).strip()
    if not name:
        return ''
    
    # ç‰¹æ®Šå¤„ç†å·²çŸ¥çš„é”€å”®ä»£è¡¨å§“å
    name_lower = name.lower()
    if name_lower == 'esther.zhu':
        return 'Esther.Zhu'
    elif name_lower == 'mia.mi':
        return 'Mia.Mi'
    
    # å¯¹äºå…¶ä»–å§“åï¼Œä¿æŒåŸæœ‰æ ¼å¼ä½†ç¡®ä¿é¦–å­—æ¯å¤§å†™
    return name

def get_normalized_sales_person(row):
    """
    ä»æ•°æ®è¡Œä¸­è·å–æ ‡å‡†åŒ–çš„é”€å”®ä»£è¡¨å§“å
    ä¼˜å…ˆä½¿ç”¨ç»­è´¹è´£ä»»é”€å”®ï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨è´£ä»»é”€å”®ä¸­è‹±æ–‡
    """
    sales_raw = row.get('ç»­è´¹è´£ä»»é”€å”®', '')
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºå€¼
    is_empty = False
    if not sales_raw or sales_raw == '' or str(sales_raw).lower() == 'nan':
        is_empty = True
    else:
        # æ£€æŸ¥æ˜¯å¦ä¸ºpandasçš„NaNå€¼
        try:
            pd = ensure_pandas_imported()
            if pd.isna(sales_raw):
                is_empty = True
        except:
            pass
    
    if is_empty:
        sales_person = str(row.get('è´£ä»»é”€å”®ä¸­è‹±æ–‡', ''))
    else:
        sales_person = str(sales_raw)
    
    return normalize_sales_name(sales_person)

# å¥åº·æ£€æŸ¥ç«¯ç‚¹
@app.route('/health')
def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹ï¼Œç”¨äºç›‘æ§æœåŠ¡çŠ¶æ€"""
    try:
        return jsonify({
            'status': 'healthy',
            'timestamp': datetime.now().isoformat(),
            'version': '1.0.0',
            'environment': os.environ.get('FLASK_ENV', 'development'),
            'services': {
                'flask': 'available',
                'template_handler': 'available' if TEMPLATE_HANDLER_AVAILABLE else 'unavailable',
                'ocr_service': 'available' if ocr_service else 'unavailable'
            }
        }), 200
    except Exception as e:
        return jsonify({
            'status': 'unhealthy',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }), 500

# æ·»åŠ å®‰å…¨å¤´
@app.after_request
def add_security_headers(response):
    if app.config.get('DEBUG'):
        # å¼€å‘ç¯å¢ƒä½¿ç”¨å®½æ¾çš„CSP
        response.headers['Content-Security-Policy'] = "default-src 'self'; script-src 'self' 'unsafe-inline' 'unsafe-eval'; style-src 'self' 'unsafe-inline'"
    else:
        # ç”Ÿäº§ç¯å¢ƒä½¿ç”¨æ›´ä¸¥æ ¼çš„å®‰å…¨å¤´
        response.headers['Content-Security-Policy'] = "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'"
        response.headers['X-Content-Type-Options'] = 'nosniff'
        response.headers['X-Frame-Options'] = 'DENY'
        response.headers['X-XSS-Protection'] = '1; mode=block'
    return response

# ç™»å½•é¡µé¢
@app.route('/login', methods=['GET', 'POST'])
def login():
    if request.method == 'POST':
        username = request.form.get('username')
        password = request.form.get('password')
        if (username == 'Esther' and password == '967420') or (username == 'Giko' and password == '549030'):  # ç®€å•çš„ç”¨æˆ·åå¯†ç éªŒè¯
            session['logged_in'] = True
            return redirect(url_for('index'))
        return render_template('login.html', error='ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯')
    return render_template('login.html')

# ç™»å‡ºåŠŸèƒ½
@app.route('/logout')
def logout():
    session.pop('logged_in', None)
    return redirect(url_for('login'))

# ç™»å½•ä¿æŠ¤è£…é¥°å™¨
def login_required(func):
    def wrapper(*args, **kwargs):
        if not session.get('logged_in'):
            return redirect(url_for('login'))
        return func(*args, **kwargs)
    wrapper.__name__ = func.__name__
    return wrapper

# åŠ è½½Excelæ•°æ®
def load_customer_data():
    global last_import_time
    excel_path = 'å…­å¤§æˆ˜åŒºç®€é“äº‘å®¢æˆ·.xlsx'
    try:
        if not os.path.exists(excel_path):
            logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {excel_path}")
            return None
        pd = ensure_pandas_imported()
        df = pd.read_excel(excel_path)
        return df
    except Exception as e:
        logger.error(f"ExcelåŠ è½½é”™è¯¯: {str(e)}")
        return None

@app.route('/')
@login_required
def index():
    return render_template('index.html', last_import_time=last_import_time)

@app.route('/test', methods=['GET'])
def test():
    return 'Hello, World!'
@app.route('/upload_excel', methods=['POST'])
@login_required
def upload_excel():
    global last_import_time
    try:
        if 'file' not in request.files:
            return jsonify({'error': 'æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶'}), 400
            
        file = request.files['file']
        if not file or not file.filename:
            return jsonify({'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'}), 400
            
        if not file.filename.endswith('.xlsx'):
            return jsonify({'error': 'è¯·ä¸Šä¼ Excelæ–‡ä»¶(.xlsx)'}), 400

        # åŸå­å†™å…¥ä¿å­˜æ–‡ä»¶ï¼šä¸´æ—¶æ–‡ä»¶ + os.replace
        target_path = os.path.join(os.getcwd(), 'å…­å¤§æˆ˜åŒºç®€é“äº‘å®¢æˆ·.xlsx')
        tmp_fd, tmp_path = tempfile.mkstemp(prefix='upload_', suffix='.xlsx', dir=os.path.dirname(target_path))
        os.close(tmp_fd)
        try:
            file.save(tmp_path)
            os.replace(tmp_path, target_path)
        except Exception as save_err:
            try:
                if os.path.exists(tmp_path):
                    os.remove(tmp_path)
            except Exception:
                pass
            logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(save_err)}")
            return jsonify({'error': f'æ–‡ä»¶ä¿å­˜å¤±è´¥: {str(save_err)}'}), 500
        
        # æ›´æ–°å¯¼å…¥æ—¶é—´
        last_import_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        return jsonify({
            'message': 'æ–‡ä»¶ä¸Šä¼ æˆåŠŸ',
            'last_import_time': last_import_time
        })
        
    except Exception as e:
        logger.error(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")
        return jsonify({'error': f'æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}'}), 500

@app.route('/get_last_import_time')
def get_last_import_time():
    return jsonify({'last_import_time': last_import_time})

# å‰ç«¯é”™è¯¯æ—¥å¿—æ¥æ”¶æ¥å£ï¼ˆæ— éœ€ç™»å½•ï¼‰
@app.route('/log_client_error', methods=['POST'])
def log_client_error():
    try:
        data = request.get_json(silent=True) or {}
        errs = data.get('errors') or []
        ua = data.get('ua') or request.headers.get('User-Agent', '')
        page = data.get('page') or request.path
        ip = request.headers.get('X-Forwarded-For', request.remote_addr)
        count = 0
        if isinstance(errs, list) and errs:
            for e in errs:
                try:
                    endpoint = e.get('endpoint')
                    msg = e.get('error')
                    ts = e.get('time')
                    logger.warning(f"å‰ç«¯é”™è¯¯: endpoint={endpoint} error={msg} time={ts} ua={ua} page={page} ip={ip}")
                    count += 1
                except Exception:
                    continue
        else:
            # å•æ¡é”™è¯¯æˆ–æ ¼å¼ä¸æ ‡å‡†
            logger.warning(f"å‰ç«¯é”™è¯¯: {errs} ua={ua} page={page} ip={ip}")
            count = 1
        return jsonify({'success': True, 'logged': count}), 200
    except Exception as exc:
        logger.error(f"è®°å½•å‰ç«¯é”™è¯¯å¤±è´¥: {str(exc)}")
        return jsonify({'success': False, 'error': 'æ—¥å¿—è®°å½•å¤±è´¥'}), 500

@app.route('/get_monthly_revenue')
@login_required
def get_monthly_revenue():
    """è·å–æœ¬æœˆæ”¶æ¬¾æ€»é‡‘é¢"""
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        excel_path = os.path.join(os.getcwd(), 'å…­å¤§æˆ˜åŒºç®€é“äº‘å®¢æˆ·.xlsx')
        logger.info(f"å°è¯•è¯»å–æ–‡ä»¶: {excel_path}")
        if not os.path.exists(excel_path):
            logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {excel_path}")
            return jsonify({'revenue': 0, 'error': 'æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨'}), 500

        try:
            ensure_pandas_imported()
            df = pd.read_excel(excel_path)
            logger.info(f"æˆåŠŸè¯»å–Excelæ–‡ä»¶ï¼Œå…±{len(df)}è¡Œæ•°æ®")
        except Exception as e:
            logger.error(f"Excelè¯»å–é”™è¯¯: {str(e)}")
            return jsonify({'revenue': 0, 'error': 'æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥'}), 500

        # è·å–å½“å‰æœˆä»½
        now = datetime.now()
        current_month = now.month
        current_year = now.year
        
        # è®¡ç®—æœ¬æœˆæ”¶æ¬¾æ€»é‡‘é¢
        monthly_revenue = 0
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ”¶æ¬¾ç›¸å…³çš„åˆ—
        revenue_columns = ['æ”¶æ¬¾é‡‘é¢', 'å›æ¬¾é‡‘é¢', 'æœ¬æœˆæ”¶æ¬¾', 'æ”¶æ¬¾', 'å›æ¬¾']
        found_column = None
        
        for col in revenue_columns:
            if col in df.columns:
                found_column = col
                break
        
        if found_column:
            # å¦‚æœæœ‰æ”¶æ¬¾æ—¥æœŸåˆ—ï¼ŒæŒ‰æœˆä»½ç­›é€‰
            if 'æ”¶æ¬¾æ—¥æœŸ' in df.columns or 'å›æ¬¾æ—¥æœŸ' in df.columns:
                date_col = 'æ”¶æ¬¾æ—¥æœŸ' if 'æ”¶æ¬¾æ—¥æœŸ' in df.columns else 'å›æ¬¾æ—¥æœŸ'
                for _, row in df.iterrows():
                    if pd.notna(row[date_col]) and pd.notna(row[found_column]):
                        try:
                            payment_date = pd.to_datetime(row[date_col])
                            if payment_date.month == current_month and payment_date.year == current_year:
                                amount = float(str(row[found_column]).replace(',', '').replace('å…ƒ', ''))
                                monthly_revenue += amount
                        except Exception as e:
                            continue
            else:
                # å¦‚æœæ²¡æœ‰æ—¥æœŸåˆ—ï¼Œä½¿ç”¨æ‰€æœ‰æœ‰æ•ˆçš„æ”¶æ¬¾é‡‘é¢
                for _, row in df.iterrows():
                    if pd.notna(row[found_column]):
                        try:
                            amount = float(str(row[found_column]).replace(',', '').replace('å…ƒ', ''))
                            monthly_revenue += amount
                        except Exception as e:
                            continue
        
        logger.info(f"æœ¬æœˆæ”¶æ¬¾æ€»é‡‘é¢: {monthly_revenue}å…ƒ")
        return jsonify({'revenue': monthly_revenue})

    except Exception as e:
        logger.error(f"è·å–æ”¶æ¬¾æ•°æ®å¤±è´¥: {str(e)}")
        return jsonify({'revenue': 0, 'error': f'è·å–æ”¶æ¬¾æ•°æ®å¤±è´¥: {str(e)}'}), 500

@app.route('/get_future_expiring_customers')
@login_required
def get_future_expiring_customers():
    try:
        # è·å–ç­›é€‰å‚æ•°ï¼šæ”¯æŒé”€å”®ç­›é€‰ï¼ˆå…¼å®¹æ—§å‚æ•°ï¼‰ä¸æˆ˜åŒºå¤šé€‰ç­›é€‰
        sales_filter = request.args.get('sales_filter', 'all')
        # å¯é€‰ï¼šæ”¯æŒè‡ªå®šä¹‰å¤©æ•°èŒƒå›´ï¼Œé»˜è®¤æ˜¾ç¤ºæœªæ¥ç¬¬8å¤©åˆ°ç¬¬33å¤©
        try:
            min_days = int(request.args.get('min_days', 8))
        except Exception:
            min_days = 8
        try:
            max_days = int(request.args.get('max_days', 33))
        except Exception:
            max_days = 33
        if max_days < min_days:
            max_days = min_days
        # æ—¢æ”¯æŒå¤šæ¬¡ä¼ å…¥ ?zones=A&zones=Bï¼Œä¹Ÿæ”¯æŒä¸€æ¬¡ä¼ å…¥ CSV ?zones=A,B
        zones_list = request.args.getlist('zones')
        if zones_list:
            # å¦‚æœåªæœ‰ä¸€ä¸ªå€¼ä¸”æ˜¯é€—å·åˆ†éš”çš„CSVï¼Œåˆ™è¿›è¡Œæ‹†åˆ†
            if len(zones_list) == 1 and isinstance(zones_list[0], str) and (',' in zones_list[0]):
                zones_list = [s.strip() for s in zones_list[0].split(',') if s.strip()]
            else:
                # å¸¸è§„å¤šå€¼å‚æ•°ï¼šç»Ÿä¸€åšå»ç©ºæ ¼å¤„ç†
                zones_list = [str(s).strip() for s in zones_list if str(s).strip()]
        else:
            zones_csv = request.args.get('zones')
            if zones_csv:
                zones_list = [s.strip() for s in str(zones_csv).split(',') if s.strip()]
        apply_zone_filter = bool(zones_list) and not (len(zones_list) == 1 and str(zones_list[0]).lower() == 'all')
        logger.info(f"è·å–æœªæ¥åˆ°æœŸå®¢æˆ·ï¼ˆ{min_days}-{max_days}å¤©ï¼‰ï¼Œé”€å”®ç­›é€‰: {sales_filter}ï¼Œæˆ˜åŒºç­›é€‰: {zones_list if apply_zone_filter else 'all'}")
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        excel_path = os.path.join(os.getcwd(), 'å…­å¤§æˆ˜åŒºç®€é“äº‘å®¢æˆ·.xlsx')
        logger.info(f"å°è¯•è¯»å–æ–‡ä»¶: {excel_path}")
        if not os.path.exists(excel_path):
            logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {excel_path}")
            return jsonify({'error': 'æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨'}), 500

        try:
            ensure_pandas_imported()
            df = pd.read_excel(excel_path)
            logger.info(f"æˆåŠŸè¯»å–Excelæ–‡ä»¶ï¼Œå…±{len(df)}è¡Œæ•°æ®")
            # åˆ—åˆ«åå…¼å®¹ï¼šåˆ°æœŸæ—¶é—´ -> åˆ°æœŸæ—¥æœŸ
            if 'åˆ°æœŸæ—¥æœŸ' not in df.columns and 'åˆ°æœŸæ—¶é—´' in df.columns:
                try:
                    df.rename(columns={'åˆ°æœŸæ—¶é—´': 'åˆ°æœŸæ—¥æœŸ'}, inplace=True)
                    logger.info("å…¼å®¹åˆ—åï¼šå°†'åˆ°æœŸæ—¶é—´'é‡å‘½åä¸º'åˆ°æœŸæ—¥æœŸ'")
                except Exception as alias_err:
                    logger.warning(f"åˆ—åå…¼å®¹å¤±è´¥: {str(alias_err)}")
            # æˆ˜åŒºåˆ—å…¼å®¹ï¼šæ”¯æŒ'æˆ˜åŒº'ã€'æ‰€å±æˆ˜åŒº'æˆ–'å½’å±æˆ˜åŒº'
            zone_col = None
            if 'æˆ˜åŒº' in df.columns:
                zone_col = 'æˆ˜åŒº'
            elif 'æ‰€å±æˆ˜åŒº' in df.columns:
                zone_col = 'æ‰€å±æˆ˜åŒº'
            elif 'å½’å±æˆ˜åŒº' in df.columns:
                zone_col = 'å½’å±æˆ˜åŒº'
        except Exception as e:
            logger.error(f"Excelè¯»å–é”™è¯¯: {str(e)}")
            return jsonify({'error': 'æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥'}), 500

        # æ£€æŸ¥å¿…è¦åˆ—å¹¶è¿”å›å…·ä½“ç¼ºå¤±é¡¹ï¼ˆé”€å”®åˆ—ä¸å†å¼ºåˆ¶è¦æ±‚ï¼‰
        required_columns = ['åˆ°æœŸæ—¥æœŸ', 'ç”¨æˆ·ID', 'è´¦å·-ä¼ä¸šåç§°']
        # å¦‚æœå¯ç”¨æˆ˜åŒºç­›é€‰ï¼Œåˆ™éœ€è¦æˆ˜åŒºåˆ—
        if apply_zone_filter and not zone_col:
            # æ˜ç¡®æç¤ºç¼ºå¤±æˆ˜åŒºåˆ—
            missing_columns = ['æˆ˜åŒº/æ‰€å±æˆ˜åŒº/å½’å±æˆ˜åŒº']
            logger.error(f"Excelæ–‡ä»¶ä¸­ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
            return jsonify({'error': f"æ•°æ®æ ¼å¼é”™è¯¯ï¼šç¼ºå°‘å¿…è¦åˆ— {missing_columns}"}), 500
        # å¸¸è§„ç¼ºåˆ—æ£€æŸ¥
        missing_columns = [col for col in required_columns if col not in df.columns]
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            logger.error(f"Excelæ–‡ä»¶ä¸­ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
            return jsonify({'error': f'æ•°æ®æ ¼å¼é”™è¯¯ï¼šç¼ºå°‘å¿…è¦åˆ— {missing_columns}'}), 500
        
        # è·å–å½“å‰æ—¥æœŸå¹¶è®¡ç®—çª—å£ï¼šé»˜è®¤æœªæ¥ç¬¬8å¤©åˆ°ç¬¬33å¤©
        now = datetime.now()
        today = now.date()
        window_start = pd.Timestamp(today) + pd.Timedelta(days=min_days)
        window_end = pd.Timestamp(today) + pd.Timedelta(days=max_days)
        
        # ç­›é€‰å‡ºæŒ‡å®šå¤©æ•°èŒƒå›´å†…ï¼ˆmin_days-max_daysï¼‰å°†è¦è¿‡æœŸçš„å®¢æˆ·
        future_customers = []

        for _, row in df.iterrows():
            if pd.notna(row['åˆ°æœŸæ—¥æœŸ']):
                try:
                    expiry_date = pd.to_datetime(row['åˆ°æœŸæ—¥æœŸ']).normalize()
                    # å¦‚æœè¿‡æœŸæ—¶é—´åœ¨min_days-max_daysçª—å£å†…
                    if window_start <= expiry_date <= window_end:
                        # ä½¿ç”¨æ ‡å‡†åŒ–å‡½æ•°è·å–é”€å”®ä»£è¡¨å§“å
                        sales_person = get_normalized_sales_person(row)
                        # æˆ˜åŒºç­›é€‰
                        if apply_zone_filter:
                            # å¦‚æœæˆ˜åŒºåˆ—ç¼ºå¤±ï¼ˆæœªåœ¨å‰é¢è¿”å›é”™è¯¯ï¼‰ï¼Œåˆ™è·³è¿‡ç­›é€‰
                            if zone_col:
                                zone_value = row.get(zone_col, '')
                                # å…¼å®¹NaNä¸å­—ç¬¦ä¸²ç©ºç™½
                                try:
                                    is_nan = pd.isna(zone_value)
                                except Exception:
                                    is_nan = False
                                zone_value_str = '' if (zone_value is None or is_nan) else str(zone_value).strip()
                                if zone_value_str not in zones_list:
                                    continue
                        else:
                            # å…¼å®¹æ—§çš„é”€å”®ç­›é€‰å‚æ•°ï¼ˆæœªå¯ç”¨æˆ˜åŒºç­›é€‰æ—¶ï¼‰
                            if sales_filter != 'all' and sales_filter != sales_person:
                                continue
                        
                        customer_info = {
                            'id': str(row.get('ç”¨æˆ·ID', '')),
                            'expiry_date': expiry_date.strftime('%Yå¹´%mæœˆ%dæ—¥'),
                            'jdy_account': str(row.get('ç”¨æˆ·ID', '')),
                            'company_name': str(row.get('è´¦å·-ä¼ä¸šåç§°', '')),
                            'sales_person': sales_person,
                            'zone': str(row.get(zone_col, '')) if zone_col else ''
                        }
                        
                        future_customers.append(customer_info)
                            
                except Exception as e:
                    logger.warning(f"æ—¥æœŸè½¬æ¢é”™è¯¯: {str(e)}")
                    continue
        
        # æŒ‰è¿‡æœŸæ—¥æœŸæ’åº
        future_customers.sort(key=lambda x: x['expiry_date'])
        
        logger.info(f"æ‰¾åˆ°{len(future_customers)}ä¸ªå³å°†è¿‡æœŸçš„å®¢æˆ·ï¼ˆé”€å”®ç­›é€‰ï¼š{sales_filter}ï¼Œæˆ˜åŒºç­›é€‰ï¼š{zones_list if apply_zone_filter else 'all'}ï¼‰")
        return jsonify({
            'future_customers': future_customers
        })

    except Exception as e:
        logger.error(f"è·å–æœªæ¥å³å°†è¿‡æœŸå®¢æˆ·å¤±è´¥: {str(e)}")
        return jsonify({'error': f'è·å–æœªæ¥å³å°†è¿‡æœŸå®¢æˆ·å¤±è´¥: {str(e)}'}), 500

@app.route('/get_sales_representatives')
@login_required
def get_sales_representatives():
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        excel_path = os.path.join(os.getcwd(), 'å…­å¤§æˆ˜åŒºç®€é“äº‘å®¢æˆ·.xlsx')
        logger.info(f"å°è¯•è¯»å–æ–‡ä»¶è·å–é”€å”®ä»£è¡¨åˆ—è¡¨: {excel_path}")
        if not os.path.exists(excel_path):
            logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {excel_path}")
            return jsonify({'error': 'æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨'}), 500

        try:
            ensure_pandas_imported()
            df = pd.read_excel(excel_path)
            logger.info(f"æˆåŠŸè¯»å–Excelæ–‡ä»¶ï¼Œå…±{len(df)}è¡Œæ•°æ®")
        except Exception as e:
            logger.error(f"Excelè¯»å–é”™è¯¯: {str(e)}")
            return jsonify({'error': 'æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥'}), 500

        # æ”¶é›†æ‰€æœ‰é”€å”®ä»£è¡¨å§“å
        sales_representatives = set()
        
        for _, row in df.iterrows():
            # ä½¿ç”¨æ ‡å‡†åŒ–å‡½æ•°è·å–é”€å”®ä»£è¡¨å§“å
            sales_person = get_normalized_sales_person(row)
            
            # æ·»åŠ åˆ°é›†åˆä¸­ï¼ˆè‡ªåŠ¨å»é‡ï¼‰
            if sales_person:
                sales_representatives.add(sales_person)
        
        # è½¬æ¢ä¸ºæ’åºåˆ—è¡¨
        sales_list = sorted(list(sales_representatives))
        
        logger.info(f"æ‰¾åˆ°{len(sales_list)}ä¸ªé”€å”®ä»£è¡¨")
        return jsonify({
            'sales_representatives': sales_list
        })

    except Exception as e:
        logger.error(f"è·å–é”€å”®ä»£è¡¨åˆ—è¡¨å¤±è´¥: {str(e)}")
        return jsonify({'error': f'è·å–é”€å”®ä»£è¡¨åˆ—è¡¨å¤±è´¥: {str(e)}'}), 500

@app.route('/get_zones')
@login_required
def get_zones():
    try:
        # é»˜è®¤æˆ˜åŒºåˆ—è¡¨ï¼ˆæŒ‰ä¸šåŠ¡å¸¸ç”¨é¡ºåºï¼‰ï¼Œæ’é™¤â€œç®€é“äº‘å¤§åŒºâ€
        default_zones_order = [
            'ä¸Šæµ·å¤§åŒº', 'ä¸œåŒ—å¤§åŒº', 'åŒ—äº¬å¤§åŒº', 'åä¸­å¤§åŒº', 'ååŒ—å¤§åŒº', 'åå—å¤§åŒº',
            'æµ™é—µå¤§åŒº', 'è‹çš–å¤§åŒº', 'è¥¿åŒ—å¤§åŒº', 'è¥¿å—å¤§åŒº'
        ]

        # ä¼˜å…ˆä»Excelæ”¶é›†æˆ˜åŒºï¼Œå¦‚æœä¸å¯ç”¨åˆ™å›é€€åˆ°é»˜è®¤åˆ—è¡¨
        excel_path = os.path.join(os.getcwd(), 'å…­å¤§æˆ˜åŒºç®€é“äº‘å®¢æˆ·.xlsx')
        logger.info(f"å°è¯•è¯»å–æ–‡ä»¶è·å–æˆ˜åŒºåˆ—è¡¨: {excel_path}")

        zones_from_excel = set()
        if os.path.exists(excel_path):
            try:
                ensure_pandas_imported()
                df = pd.read_excel(excel_path)
                logger.info(f"æˆåŠŸè¯»å–Excelæ–‡ä»¶ï¼Œå…±{len(df)}è¡Œæ•°æ®")

                # æˆ˜åŒºåˆ—å…¼å®¹ï¼šæ”¯æŒ'æˆ˜åŒº'ã€'æ‰€å±æˆ˜åŒº'æˆ–'å½’å±æˆ˜åŒº'
                zone_col = None
                if 'æˆ˜åŒº' in df.columns:
                    zone_col = 'æˆ˜åŒº'
                elif 'æ‰€å±æˆ˜åŒº' in df.columns:
                    zone_col = 'æ‰€å±æˆ˜åŒº'
                elif 'å½’å±æˆ˜åŒº' in df.columns:
                    zone_col = 'å½’å±æˆ˜åŒº'

                if zone_col:
                    for _, row in df.iterrows():
                        zone_val = row.get(zone_col, '')
                        try:
                            is_nan = pd.isna(zone_val)
                        except Exception:
                            is_nan = False
                        if zone_val is None or is_nan:
                            continue
                        zone_str = str(zone_val).strip()
                        if zone_str and zone_str != 'ç®€é“äº‘å¤§åŒº':
                            zones_from_excel.add(zone_str)
                else:
                    logger.warning("Excelæ–‡ä»¶ä¸­ç¼ºå°‘æˆ˜åŒºåˆ— ['æˆ˜åŒº'ã€'æ‰€å±æˆ˜åŒº'ã€'å½’å±æˆ˜åŒº']ï¼Œå°†ä»…ä½¿ç”¨é»˜è®¤æˆ˜åŒºåˆ—è¡¨")
            except Exception as e:
                logger.warning(f"Excelè¯»å–æˆ–è§£ææˆ˜åŒºå¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤æˆ˜åŒºåˆ—è¡¨: {str(e)}")
        else:
            logger.warning(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {excel_path}ï¼Œå°†ä½¿ç”¨é»˜è®¤æˆ˜åŒºåˆ—è¡¨")

        # åˆå¹¶é»˜è®¤æˆ˜åŒºä¸Excelæˆ˜åŒºï¼Œä¿æŒé»˜è®¤é¡ºåºï¼Œå…¶ä½™è¿½åŠ åœ¨å
        merged_zones = []
        seen = set()
        for z in default_zones_order:
            if z not in seen:
                merged_zones.append(z)
                seen.add(z)
        for z in sorted(zones_from_excel):
            if z not in seen:
                merged_zones.append(z)
                seen.add(z)

        logger.info(f"è¿”å›{len(merged_zones)}ä¸ªæˆ˜åŒºï¼ˆå«é»˜è®¤ä¸Excelæå–ï¼Œå·²æ’é™¤ç®€é“äº‘å¤§åŒºï¼‰")
        return jsonify({'zones': merged_zones})

    except Exception as e:
        logger.error(f"è·å–æˆ˜åŒºåˆ—è¡¨å¤±è´¥: {str(e)}")
        return jsonify({'error': f'è·å–æˆ˜åŒºåˆ—è¡¨å¤±è´¥: {str(e)}'}), 500

@app.route('/get_unsigned_customers')
@login_required
def get_unsigned_customers():
    """è·å–æœªæ¥8-33å¤©å†…å®¢æˆ·æ•°æ®ï¼Œæ”¯æŒçŠ¶æ€ç­›é€‰"""
    try:
        # è·å–ç­›é€‰å‚æ•°
        status_filter = request.args.get('status', 'all')  # all, na, contract, invoice, paid
        # å¯é€‰ï¼šæ”¯æŒè‡ªå®šä¹‰å¤©æ•°èŒƒå›´ï¼Œé»˜è®¤æœªæ¥ç¬¬8å¤©åˆ°ç¬¬33å¤©
        try:
            min_days = int(request.args.get('min_days', 8))
        except Exception:
            min_days = 8
        try:
            max_days = int(request.args.get('max_days', 33))
        except Exception:
            max_days = 33
        if max_days < min_days:
            max_days = min_days
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        excel_path = os.path.join(os.getcwd(), 'å…­å¤§æˆ˜åŒºç®€é“äº‘å®¢æˆ·.xlsx')
        logger.info(f"å°è¯•è¯»å–æ–‡ä»¶: {excel_path}")
        if not os.path.exists(excel_path):
            logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {excel_path}")
            return jsonify({'customers': [], 'error': 'æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨'}), 500

        try:
            ensure_pandas_imported()
            df = pd.read_excel(excel_path)
            logger.info(f"æˆåŠŸè¯»å–Excelæ–‡ä»¶ï¼Œå…±{len(df)}è¡Œæ•°æ®")
            # åˆ—åˆ«åå…¼å®¹ï¼šéƒ¨åˆ†æ•°æ®ä½¿ç”¨â€œåˆ°æœŸæ—¶é—´â€ï¼Œç»Ÿä¸€é‡å‘½åä¸ºâ€œåˆ°æœŸæ—¥æœŸâ€
            if 'åˆ°æœŸæ—¥æœŸ' not in df.columns and 'åˆ°æœŸæ—¶é—´' in df.columns:
                try:
                    df.rename(columns={'åˆ°æœŸæ—¶é—´': 'åˆ°æœŸæ—¥æœŸ'}, inplace=True)
                    logger.info("å…¼å®¹åˆ—åï¼šå°†'åˆ°æœŸæ—¶é—´'é‡å‘½åä¸º'åˆ°æœŸæ—¥æœŸ'")
                except Exception as alias_err:
                    logger.warning(f"åˆ—åå…¼å®¹å¤±è´¥: {str(alias_err)}")
        except Exception as e:
            logger.error(f"Excelè¯»å–é”™è¯¯: {str(e)}")
            return jsonify({'customers': [], 'error': 'æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥'}), 500

        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨ï¼ˆåˆ°æœŸæ—¥æœŸå·²åœ¨ä¸Šæ–¹åšè¿‡åˆ«åå…¼å®¹ï¼‰
        required_columns = ['ç”¨æˆ·ID', 'è´¦å·-ä¼ä¸šåç§°', 'åˆ°æœŸæ—¥æœŸ', 'å®¢æˆ·é˜¶æ®µ']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            logger.error(f"Excelæ–‡ä»¶ä¸­ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
            return jsonify({'customers': [], 'error': f'æ•°æ®æ ¼å¼é”™è¯¯ï¼šç¼ºå°‘å¿…è¦åˆ— {missing_columns}'}), 500
        
        # è·å–å½“å‰æ—¥æœŸ
        now = datetime.now()
        today = now.date()
        eight_days_later = today + pd.Timedelta(days=min_days)
        thirty_three_days_later = today + pd.Timedelta(days=max_days)
        
        # ç­›é€‰å‡ºæœªæ¥30å¤©å†…åˆ°æœŸçš„å®¢æˆ·
        filtered_customers = []
        for _, row in df.iterrows():
            # æ£€æŸ¥åˆ°æœŸæ—¥æœŸæ˜¯å¦åœ¨æœªæ¥8-33å¤©å†…
            if pd.notna(row['åˆ°æœŸæ—¥æœŸ']):
                try:
                    expiry_date = pd.to_datetime(row['åˆ°æœŸæ—¥æœŸ']).date()
                    # å¦‚æœåˆ°æœŸæ—¥æœŸåœ¨æœªæ¥8-33å¤©å†…
                    if eight_days_later <= expiry_date <= thirty_three_days_later:
                        # è·å–å®¢æˆ·é˜¶æ®µ
                        customer_stage = row.get('å®¢æˆ·é˜¶æ®µ', '')
                        stage_normalized = ''
                        if pd.notna(customer_stage) and str(customer_stage).strip() != '' and str(customer_stage).lower() != 'nan':
                            stage_normalized = str(customer_stage).strip()
                        
                        # æ ¹æ®çŠ¶æ€ç­›é€‰
                        should_include = False
                        if status_filter == 'all':
                            should_include = True
                        # NA å…¼å®¹ï¼šç©ºå€¼æˆ–æ˜¾å¼å­—ç¬¦ä¸² "NA"
                        elif status_filter == 'na' and (stage_normalized == '' or str(stage_normalized).strip().upper() == 'NA'):
                            should_include = True
                        elif status_filter == 'contract' and 'åˆåŒ' in stage_normalized:
                            should_include = True
                        elif status_filter == 'invoice' and 'å¼€ç¥¨' in stage_normalized:
                            should_include = True
                        elif status_filter == 'advance_invoice' and 'æå‰å¼€' in stage_normalized:
                            should_include = True
                        elif status_filter == 'paid' and ('å›æ¬¾' in stage_normalized or 'å·²ä»˜' in stage_normalized):
                            should_include = True
                        elif status_filter == 'upsell' and 'å¢è´­' in stage_normalized:
                            should_include = True
                        elif status_filter == 'invalid' and 'æ— æ•ˆ' in stage_normalized:
                            should_include = True
                        elif status_filter == 'lost' and 'å¤±è”' in stage_normalized:
                            should_include = True
                        
                        if should_include:
                            # å¤„ç†è´£ä»»é”€å”®å­—æ®µ
                            sales_raw = row.get('ç»­è´¹è´£ä»»é”€å”®', '')
                            if pd.isna(sales_raw) or sales_raw == '' or str(sales_raw).lower() == 'nan':
                                sales_person = str(row.get('è´£ä»»é”€å”®ä¸­è‹±æ–‡', ''))
                            else:
                                sales_person = str(sales_raw)
                            
                            # è®¡ç®—è·ç¦»åˆ°æœŸçš„å¤©æ•°
                            days_until_expiry = (expiry_date - today).days
                            if days_until_expiry == 0:
                                date_label = "ä»Šå¤©åˆ°æœŸ"
                            elif days_until_expiry == 1:
                                date_label = "æ˜å¤©åˆ°æœŸ"
                            else:
                                date_label = f"{days_until_expiry}å¤©ååˆ°æœŸ"
                            
                            filtered_customers.append({
                                'expiry_date': f"{date_label} ({expiry_date.strftime('%Yå¹´%mæœˆ%dæ—¥')})",
                                'jdy_account': str(row.get('ç”¨æˆ·ID', '')),
                                'company_name': str(row.get('è´¦å·-ä¼ä¸šåç§°', '')),
                                'sales_person': sales_person,
                                'customer_stage': stage_normalized if stage_normalized else 'NA',
                                'days_until_expiry': days_until_expiry
                            })
                except Exception as e:
                    logger.warning(f"æ—¥æœŸè½¬æ¢é”™è¯¯: {str(e)}")
                    continue
        
        # æŒ‰åˆ°æœŸæ—¥æœŸæ’åºï¼ˆæœ€è¿‘åˆ°æœŸçš„åœ¨å‰ï¼‰
        filtered_customers.sort(key=lambda x: x['days_until_expiry'])
        
        # è·å–æ‰€æœ‰æœªæ¥8-33å¤©å†…çš„å®¢æˆ·ï¼ˆä¸è€ƒè™‘çŠ¶æ€ç­›é€‰ï¼‰ç”¨äºè®¡ç®—å„çŠ¶æ€çš„æ•°é‡
        all_customers_30days = []
        for _, row in df.iterrows():
            if pd.notna(row['åˆ°æœŸæ—¥æœŸ']):
                try:
                    expiry_date = pd.to_datetime(row['åˆ°æœŸæ—¥æœŸ']).date()
                    if eight_days_later <= expiry_date <= thirty_three_days_later:
                        customer_stage = row.get('å®¢æˆ·é˜¶æ®µ', '')
                        stage_normalized = ''
                        if pd.notna(customer_stage) and str(customer_stage).strip() != '' and str(customer_stage).lower() != 'nan':
                            stage_normalized = str(customer_stage).strip()
                        
                        all_customers_30days.append({
                            'customer_stage': stage_normalized if stage_normalized else 'NA'
                        })
                except Exception:
                    continue
        
        # è®¡ç®—å„çŠ¶æ€è®¡æ•°ï¼ˆç¨³å®šè¿”å›æ‰€æœ‰å·²çŸ¥çŠ¶æ€ï¼Œå³ä½¿ä¸º0ï¼‰
        count_all = len(all_customers_30days)
        count_na = len([c for c in all_customers_30days if str(c['customer_stage']).strip().upper() == 'NA'])
        count_contract = len([c for c in all_customers_30days if 'åˆåŒ' in c['customer_stage']])
        count_invoice = len([c for c in all_customers_30days if 'å¼€ç¥¨' in c['customer_stage']])
        count_advance_invoice = len([c for c in all_customers_30days if 'æå‰å¼€' in c['customer_stage']])
        count_paid = len([c for c in all_customers_30days if ('å›æ¬¾' in c['customer_stage'] or 'å·²ä»˜' in c['customer_stage'])])
        count_upsell = len([c for c in all_customers_30days if 'å¢è´­' in c['customer_stage']])
        count_invalid = len([c for c in all_customers_30days if 'æ— æ•ˆ' in c['customer_stage']])
        count_lost = len([c for c in all_customers_30days if 'å¤±è”' in c['customer_stage']])

        # å›ºå®šé¡ºåºè¿”å›ï¼Œé¿å…å‰ç«¯èŠ¯ç‰‡ç¼ºå¤±
        unique_statuses = [
            {'value': 'all', 'label': 'å…¨éƒ¨çŠ¶æ€', 'count': count_all},
            {'value': 'na', 'label': 'NAçŠ¶æ€', 'count': count_na},
            {'value': 'contract', 'label': 'åˆåŒçŠ¶æ€', 'count': count_contract},
            {'value': 'invoice', 'label': 'å¼€ç¥¨çŠ¶æ€', 'count': count_invoice},
            {'value': 'advance_invoice', 'label': 'æå‰å¼€çŠ¶æ€', 'count': count_advance_invoice},
            {'value': 'paid', 'label': 'å›æ¬¾çŠ¶æ€', 'count': count_paid},
            {'value': 'upsell', 'label': 'å¢è´­çŠ¶æ€', 'count': count_upsell},
            {'value': 'invalid', 'label': 'æ— æ•ˆçŠ¶æ€', 'count': count_invalid},
            {'value': 'lost', 'label': 'å¤±è”çŠ¶æ€', 'count': count_lost},
        ]
        
        logger.info(f"æ‰¾åˆ°{len(filtered_customers)}ä¸ªæœªæ¥{min_days}-{max_days}å¤©å†…çš„å®¢æˆ·ï¼ˆç­›é€‰æ¡ä»¶: {status_filter}ï¼‰")
        return jsonify({
            'customers': filtered_customers,
            'total_count': len(filtered_customers),
            'query_date': today.strftime('%Yå¹´%mæœˆ%dæ—¥'),
            'current_filter': status_filter,
            'available_statuses': unique_statuses
        })

    except Exception as e:
        logger.error(f"è·å–å®¢æˆ·æ•°æ®å¤±è´¥: {str(e)}")
        return jsonify({
            'customers': [], 
            'error': f'è·å–å®¢æˆ·ä¿¡æ¯æ—¶å‡ºç°é—®é¢˜',
            'query_date': datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
        }), 500

@app.route('/export_unsigned_customers')
@login_required
def export_unsigned_customers():
    """å¯¼å‡ºæ‰€æœ‰å®¢æˆ·æ•°æ®ï¼ŒåŒ…å«å…¨éƒ¨3606è¡Œ"""
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        excel_path = os.path.join(os.getcwd(), 'å…­å¤§æˆ˜åŒºç®€é“äº‘å®¢æˆ·.xlsx')
        logger.info(f"å°è¯•è¯»å–æ–‡ä»¶: {excel_path}")
        if not os.path.exists(excel_path):
            logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {excel_path}")
            return jsonify({'error': 'æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨'}), 500

        try:
            ensure_pandas_imported()
            df = pd.read_excel(excel_path)
            logger.info(f"æˆåŠŸè¯»å–Excelæ–‡ä»¶ï¼Œå…±{len(df)}è¡Œæ•°æ®")
        except Exception as e:
            logger.error(f"Excelè¯»å–é”™è¯¯: {str(e)}")
            return jsonify({'error': 'æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥'}), 500

        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
        required_columns = ['ç”¨æˆ·ID', 'è´¦å·-ä¼ä¸šåç§°', 'åˆ°æœŸæ—¥æœŸ', 'å®¢æˆ·é˜¶æ®µ']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            logger.error(f"Excelæ–‡ä»¶ä¸­ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
            return jsonify({'error': f'æ•°æ®æ ¼å¼é”™è¯¯ï¼šç¼ºå°‘å¿…è¦åˆ— {missing_columns}'}), 500
        
        # è·å–å½“å‰æ—¥æœŸ
        now = datetime.now()
        today = now.date()
        
        # åˆ›å»ºå¯¼å‡ºçš„DataFrameï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®
        export_df = df.copy()
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        temp_file = tempfile.NamedTemporaryFile(suffix='.xlsx', delete=False)
        
        # ä¿å­˜ä¸ºExcelæ–‡ä»¶
        export_df.to_excel(temp_file.name, index=False)
        
        logger.info(f"å¯¼å‡ºäº†{len(export_df)}ä¸ªå®¢æˆ·æ•°æ®åˆ°ä¸´æ—¶æ–‡ä»¶")
        
        # è¿”å›æ–‡ä»¶ä¸‹è½½
        return send_file(
            temp_file.name,
            as_attachment=True,
            download_name=f"å…­å¤§æˆ˜åŒºå…¨éƒ¨å®¢æˆ·_{today.strftime('%Y%m%d')}.xlsx",
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )
        
    except Exception as e:
        logger.error(f"å¯¼å‡ºå®¢æˆ·æ•°æ®å¤±è´¥: {str(e)}")
        return jsonify({'error': f'å¯¼å‡ºå®¢æˆ·æ•°æ®æ—¶å‡ºç°é—®é¢˜: {str(e)}'}), 500
    finally:
        # ç¡®ä¿ä¸´æ—¶æ–‡ä»¶è¢«æ¸…ç†
        try:
            if 'temp_file' in locals():
                os.unlink(temp_file.name)
        except Exception as e:
            logger.error(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

@app.route('/get_expiring_customers')
@login_required
def get_expiring_customers():
    try:
        # è·å–ç­›é€‰å‚æ•°
        sales_filter = request.args.get('sales_filter', 'all')
        test_mode = request.args.get('test_mode', 'false').lower() == 'true'
        # æˆ˜åŒºç­›é€‰å‚æ•°ï¼ˆæ”¯æŒCSVå’Œé‡å¤å‚æ•°ä¸¤ç§å½¢å¼ï¼‰
        raw_zones = request.args.getlist('zones')
        zones_list = []
        for val in raw_zones:
            if not val:
                continue
            if ',' in val:
                for z in val.split(','):
                    z_clean = z.strip()
                    if z_clean:
                        zones_list.append(z_clean)
            else:
                v = val.strip()
                if v:
                    zones_list.append(v)
        # å»é‡å¹¶ä¿ç•™é¡ºåº
        zones_list = list(dict.fromkeys(zones_list))
        logger.info(f"=== APIè°ƒç”¨å¼€å§‹ ===")
        logger.info(f"è¯·æ±‚å‚æ•° - sales_filter: {sales_filter}, test_mode: {test_mode}, zones: {zones_list}")
        logger.info(f"åŸå§‹å‚æ•° - sales_filter: {request.args.get('sales_filter')}, test_mode: {request.args.get('test_mode')}, zones(raw): {raw_zones}")
        logger.info(f"è·å–åˆ°æœŸå®¢æˆ·ï¼Œé”€å”®ç­›é€‰: {sales_filter}, æˆ˜åŒºç­›é€‰: {zones_list}, æµ‹è¯•æ¨¡å¼: {test_mode}")
        
        # è·å–å½“å‰æ—¥æœŸ
        now = datetime.now()
        today = now.date()
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        excel_path = os.path.join(os.getcwd(), 'å…­å¤§æˆ˜åŒºç®€é“äº‘å®¢æˆ·.xlsx')
        logger.info(f"å°è¯•è¯»å–æ–‡ä»¶: {excel_path}")
        if not os.path.exists(excel_path):
            logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {excel_path}")
            return jsonify({'expiring_customers': [], 'error': 'æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨', 'today_date': today.strftime('%Yå¹´%mæœˆ%dæ—¥')})

        try:
            ensure_pandas_imported()
            df = pd.read_excel(excel_path)
            logger.info(f"æˆåŠŸè¯»å–Excelæ–‡ä»¶ï¼Œå…±{len(df)}è¡Œæ•°æ®")
            # åˆ—åˆ«åå…¼å®¹ï¼šåˆ°æœŸæ—¶é—´ -> åˆ°æœŸæ—¥æœŸ
            if 'åˆ°æœŸæ—¥æœŸ' not in df.columns and 'åˆ°æœŸæ—¶é—´' in df.columns:
                try:
                    df.rename(columns={'åˆ°æœŸæ—¶é—´': 'åˆ°æœŸæ—¥æœŸ'}, inplace=True)
                    logger.info("å…¼å®¹åˆ—åï¼šå°†'åˆ°æœŸæ—¶é—´'é‡å‘½åä¸º'åˆ°æœŸæ—¥æœŸ'")
                except Exception as alias_err:
                    logger.warning(f"åˆ—åå…¼å®¹å¤±è´¥: {str(alias_err)}")
        except Exception as e:
            logger.error(f"Excelè¯»å–é”™è¯¯: {str(e)}")
            return jsonify({'expiring_customers': [], 'error': 'æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥', 'today_date': today.strftime('%Yå¹´%mæœˆ%dæ—¥')})

        # æ£€æŸ¥å¿…è¦çš„åˆ—å¹¶è¿”å›å…·ä½“ç¼ºå¤±é¡¹
        required_columns = ['åˆ°æœŸæ—¥æœŸ', 'ç”¨æˆ·ID', 'è´¦å·-ä¼ä¸šåç§°']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            logger.error(f"Excelæ–‡ä»¶ä¸­ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
            return jsonify({
                'expiring_customers': [],
                'error': f'æ•°æ®æ ¼å¼é”™è¯¯ï¼šç¼ºå°‘å¿…è¦åˆ— {missing_columns}',
                'today_date': today.strftime('%Yå¹´%mæœˆ%dæ—¥')
            })
        
        # æˆ˜åŒºåˆ—è¯†åˆ«
        zone_col = None
        for candidate in ['æˆ˜åŒº', 'æ‰€å±æˆ˜åŒº', 'å½’å±æˆ˜åŒº']:
            if candidate in df.columns:
                zone_col = candidate
                break
        apply_zone_filter = len(zones_list) > 0 and zone_col is not None
        if len(zones_list) > 0 and zone_col is None:
            logger.warning("Excelä¸­æœªæ‰¾åˆ°æˆ˜åŒºç›¸å…³åˆ—ï¼Œå¿½ç•¥æˆ˜åŒºç­›é€‰")
        
        # æŒ‰éœ€æ±‚ï¼šæ˜¾ç¤ºä»ä»Šå¤©å¼€å§‹ï¼Œå‘åæ¨å»¶7å¤©å†…åˆ°æœŸ
        start_date = today
        end_date = today + pd.Timedelta(days=7)
        reminder_type = "æœªæ¥7å¤©åˆ°æœŸæé†’"
        logger.info(f"åˆ°æœŸæé†’çª—å£ï¼š{start_date} è‡³ {end_date}ï¼Œæˆ˜åŒºåˆ—: {zone_col}")
        
        # ç­›é€‰å‡ºç›®æ ‡æ—¥æœŸåˆ°æœŸçš„å®¢æˆ·
        expiring_customers = []
        total_expiring = 0
        filtered_out = 0
        
        for _, row in df.iterrows():
            if pd.notna(row['åˆ°æœŸæ—¥æœŸ']):
                try:
                    expiry_date = pd.to_datetime(row['åˆ°æœŸæ—¥æœŸ']).date()
                    if start_date <= expiry_date <= end_date:
                        total_expiring += 1
                        # ä½¿ç”¨æ ‡å‡†åŒ–å‡½æ•°è·å–é”€å”®ä»£è¡¨å§“å
                        sales_person = get_normalized_sales_person(row)
                        
                        # åº”ç”¨é”€å”®ä»£è¡¨ç­›é€‰
                        if sales_filter != 'all' and sales_filter != sales_person:
                            filtered_out += 1
                            continue
                        
                        # è¯»å–æˆ˜åŒºç”¨äºå±•ç¤º/ç­›é€‰
                        zone_val = ''
                        if zone_col is not None:
                            raw_zone_val = row.get(zone_col, None)
                            try:
                                is_nan_zone = pd.isna(raw_zone_val)
                            except Exception:
                                is_nan_zone = False
                            zone_val = '' if (raw_zone_val is None or is_nan_zone) else str(raw_zone_val).strip()
                        
                        # åº”ç”¨æˆ˜åŒºç­›é€‰ï¼ˆå¦‚æœ‰ï¼‰
                        if apply_zone_filter:
                            if zone_val == '' or zone_val not in zones_list:
                                filtered_out += 1
                                continue
                        
                        # è®¡ç®—è·ç¦»åˆ°æœŸçš„å¤©æ•°
                        days_until_expiry = (expiry_date - today).days
                        if days_until_expiry == 0:
                            date_label = "ä»Šå¤©åˆ°æœŸ"
                        elif days_until_expiry == 1:
                            date_label = "æ˜å¤©åˆ°æœŸ"
                        else:
                            date_label = f"{days_until_expiry}å¤©ååˆ°æœŸ"
                        # è¿½åŠ å®¢æˆ·é˜¶æ®µåˆ°æ—¥æœŸæ ‡ç­¾ï¼ˆå¦‚æœ‰ï¼‰ï¼Œæ ¼å¼ç¤ºä¾‹ï¼š3å¤©ååˆ°æœŸ-å›æ¬¾ (2025å¹´10æœˆ31æ—¥)
                        stage_val = row.get('å®¢æˆ·é˜¶æ®µ', None)
                        try:
                            stage_is_nan = pd.isna(stage_val)
                        except Exception:
                            stage_is_nan = False
                        stage_label = '' if (stage_val is None or stage_is_nan) else str(stage_val).strip()
                        if stage_label:
                            expiry_text = f"{date_label}-{stage_label} ({expiry_date.strftime('%Yå¹´%mæœˆ%dæ—¥')})"
                        else:
                            expiry_text = f"{date_label} ({expiry_date.strftime('%Yå¹´%mæœˆ%dæ—¥')})"

                        expiring_customers.append({
                            'expiry_date': expiry_text,
                            'jdy_account': str(row.get('ç”¨æˆ·ID', '')),
                            'company_name': str(row.get('è´¦å·-ä¼ä¸šåç§°', '')),
                            'sales_person': sales_person,
                            'customer_classification': str(row.get('å®¢æˆ·åˆ†ç±»', '')),
                            'days_until_expiry': days_until_expiry,
                            'zone': zone_val
                        })
                except Exception as e:
                    logger.warning(f"æ—¥æœŸè½¬æ¢é”™è¯¯: {str(e)}")
                    continue
        
        # æŒ‰åˆ°æœŸæ—¥æœŸæ’åº
        expiring_customers.sort(key=lambda x: x['days_until_expiry'])
        
        # è®°å½•ç­›é€‰ç»Ÿè®¡ä¿¡æ¯
        logger.info(f"ç­›é€‰ç»Ÿè®¡ - æ€»åˆ°æœŸå®¢æˆ·: {total_expiring}, ç­›é€‰æ‰: {filtered_out}, æœ€ç»ˆç»“æœ: {len(expiring_customers)}, ç­›é€‰æ¡ä»¶: {sales_filter}")
        
        if len(expiring_customers) == 0:
            # æœªæ¥7å¤©çª—å£çš„æç¤ºä¿¡æ¯ï¼ˆæ ¹æ®æ˜¯å¦æœ‰æˆ˜åŒºç­›é€‰ï¼‰
            if apply_zone_filter and len(zones_list) > 0:
                zones_str = 'ï¼Œ'.join(zones_list)
                message = f"ğŸ˜Š {zones_str}åœ¨æœªæ¥7å¤©å†…æ²¡æœ‰å®¢æˆ·åˆ°æœŸ"
            else:
                message = "ğŸ˜Š æœªæ¥7å¤©å†…æ²¡æœ‰å®¢æˆ·åˆ°æœŸ"
            logger.info(message)
            return jsonify({
                'expiring_customers': [], 
                'message': message,
                'today_date': today.strftime('%Yå¹´%mæœˆ%dæ—¥'),
                'reminder_type': reminder_type,
                'selected_zones': zones_list
            })
        else:
            logger.info(f"æ‰¾åˆ°{len(expiring_customers)}ä¸ªå³å°†è¿‡æœŸçš„å®¢æˆ·")
            return jsonify({
                'expiring_customers': expiring_customers,
                'today_date': today.strftime('%Yå¹´%mæœˆ%dæ—¥'),
                'reminder_type': reminder_type,
                'selected_zones': zones_list
            })

    except Exception as e:
        logger.error(f"è·å–å³å°†è¿‡æœŸå®¢æˆ·å¤±è´¥: {str(e)}")
        return jsonify({
            'expiring_customers': [], 
            'error': f'è·å–å®¢æˆ·ä¿¡æ¯æ—¶å‡ºç°é—®é¢˜',
            'today_date': datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥'),
            'reminder_type': 'ç³»ç»Ÿé”™è¯¯'
        })

@app.route('/query_customer', methods=['POST'])
@login_required
def query_customer():
    try:
        if not request.json or ('jdy_id' not in request.json and 'company_name' not in request.json):
            logger.warning("è¯·æ±‚ä¸­ç¼ºå°‘æŸ¥è¯¢å‚æ•°")
            return jsonify({'error': 'è¯·æä¾›ç®€é“äº‘è´¦å·æˆ–å…¬å¸åç§°'}), 400
            
        # è·å–æŸ¥è¯¢å‚æ•°
        jdy_id = request.json.get('jdy_id', '')
        company_name = request.json.get('company_name', '')
        
        # è®°å½•æŸ¥è¯¢å‚æ•°
        if jdy_id:
            logger.info(f"å¼€å§‹é€šè¿‡ç®€é“äº‘è´¦å·æŸ¥è¯¢: {jdy_id}")
        if company_name:
            logger.info(f"å¼€å§‹é€šè¿‡å…¬å¸åç§°æŸ¥è¯¢: {company_name}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        excel_path = 'å…­å¤§æˆ˜åŒºç®€é“äº‘å®¢æˆ·.xlsx'
        if not os.path.exists(excel_path):
            logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {excel_path}")
            return jsonify({'error': 'æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨'}), 500

        try:
            # ç¡®ä¿pandaså·²å»¶è¿Ÿå¯¼å…¥
            pd = ensure_pandas_imported()
            df = pd.read_excel(excel_path)
            logger.info(f"æˆåŠŸè¯»å–Excelæ–‡ä»¶ï¼Œå…±{len(df)}è¡Œæ•°æ®")
        except Exception as e:
            logger.error(f"Excelè¯»å–é”™è¯¯: {str(e)}")
            return jsonify({'error': 'æ•°æ®æ–‡ä»¶è¯»å–å¤±è´¥'}), 500

        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
        required_columns = ['ç”¨æˆ·ID', 'å…¬å¸åç§°']
        for col in required_columns:
            if col not in df.columns:
                logger.error(f"Excelæ–‡ä»¶ä¸­ç¼ºå°‘'{col}'åˆ—")
                return jsonify({'error': f'æ•°æ®æ ¼å¼é”™è¯¯ï¼šç¼ºå°‘{col}åˆ—'}), 500
        
        # æ ¹æ®æŸ¥è¯¢æ¡ä»¶è¿›è¡Œæ¨¡ç³ŠåŒ¹é…
        if jdy_id:
            matching_rows = df[df['ç”¨æˆ·ID'].astype(str).str.contains(str(jdy_id), case=False, na=False)]
        else:
            # ä¼˜åŒ–ï¼šåŒæ—¶åœ¨"å…¬å¸åç§°"å’Œ"è´¦å·-ä¼ä¸šåç§°"åˆ—ä¸­è¿›è¡Œæœç´¢
            company_name_lower = str(company_name).lower()
            # ä½¿ç”¨ORæ¡ä»¶è¿›è¡Œå¤šåˆ—æœç´¢
            matching_rows = df[
                (df['å…¬å¸åç§°'].astype(str).str.lower().str.contains(company_name_lower, na=False)) |
                (df['è´¦å·-ä¼ä¸šåç§°'].astype(str).str.lower().str.contains(company_name_lower, na=False))
            ]
            
        if matching_rows.empty:
            query_type = "ç®€é“äº‘è´¦å·" if jdy_id else "å…¬å¸åç§°"
            query_value = jdy_id if jdy_id else company_name
            logger.info(f"æœªæ‰¾åˆ°åŒ¹é…çš„å®¢æˆ·ä¿¡æ¯ï¼ŒæŸ¥è¯¢{query_type}: {query_value}")
            return jsonify({'error': 'æœªæ‰¾åˆ°å®¢æˆ·ä¿¡æ¯'}), 404

        # å¤„ç†å¤šæ¡åŒ¹é…è®°å½•
        results = []
        for _, customer_data in matching_rows.iterrows():
            # å¤„ç†æ–°å­—æ®µæ˜ å°„
            account_enterprise_name = str(customer_data.get('è´¦å·-ä¼ä¸šåç§°', ''))
            integration_mode = str(customer_data.get('é›†æˆæ¨¡å¼', ''))
            customer_classification = str(customer_data.get('å®¢æˆ·åˆ†ç±»', ''))
            
            # å¤„ç†è´£ä»»é”€å”®å­—æ®µ - ä¼˜å…ˆä½¿ç”¨ç»­è´¹è´£ä»»é”€å”®ï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨è´£ä»»é”€å”®ä¸­è‹±æ–‡
            sales_raw = customer_data.get('ç»­è´¹è´£ä»»é”€å”®', '')
            if pd.isna(sales_raw) or sales_raw == '' or str(sales_raw).lower() == 'nan':
                sales = str(customer_data.get('è´£ä»»é”€å”®ä¸­è‹±æ–‡', ''))
            else:
                sales = str(sales_raw)
            
            sales_cn_en = str(customer_data.get('è´£ä»»é”€å”®ä¸­è‹±æ–‡', ''))
            jdy_sales = str(customer_data.get('ç®€é“äº‘é”€å”®', ''))
            
            logger.info(f"å¤„ç†å®¢æˆ·æ•°æ®: {customer_data['ç”¨æˆ·ID']}, è´¦å·-ä¼ä¸šåç§°: {account_enterprise_name}, "
                      f"é›†æˆæ¨¡å¼: {integration_mode}, å®¢æˆ·åˆ†ç±»: {customer_classification}, "
                      f"ç»­è´¹è´£ä»»é”€å”®: {sales}, è´£ä»»é”€å”®ä¸­è‹±æ–‡: {sales_cn_en}, ç®€é“äº‘é”€å”®: {jdy_sales}")
            
            # å¤„ç†åˆ°æœŸæ—¥æœŸ
            expiry_date = ''
            if 'åˆ°æœŸæ—¥æœŸ' in customer_data and pd.notna(customer_data['åˆ°æœŸæ—¥æœŸ']):
                try:
                    expiry_date = pd.to_datetime(customer_data['åˆ°æœŸæ—¥æœŸ']).strftime('%Yå¹´%mæœˆ%dæ—¥')
                    logger.info(f"åˆ°æœŸæ—¥æœŸ: {expiry_date}")
                except Exception as e:
                    logger.warning(f"æ—¥æœŸè½¬æ¢é”™è¯¯: {str(e)}")
                    expiry_date = ''
            
            # å¤„ç†ARR
            try:
                arr_value = customer_data.get('åº”ç»­ARR', 0)
                if pd.isna(arr_value) or arr_value == '' or float(str(arr_value).replace(',', '')) == 0:
                    arr_display = '0å…ƒ'
                else:
                    arr_display = f"{float(str(arr_value).replace(',', ''))}å…ƒ"
                logger.info(f"åº”ç»­ARR: {arr_display}")
            except Exception as e:
                logger.warning(f"ARRå¤„ç†é”™è¯¯: {str(e)}")
                arr_display = '0å…ƒ'
            
            results.append({
                'account_enterprise_name': account_enterprise_name,  # è´¦å·-ä¼ä¸šåç§°
                'company_name': str(customer_data.get('å…¬å¸åç§°', '')),  # å…¬å¸åç§°
                'tax_number': str(customer_data.get('ç¨å·', '')),  # ç¨å·
                'integration_mode': integration_mode,  # é›†æˆæ¨¡å¼
                'expiry_date': expiry_date,  # åˆ°æœŸæ—¥æœŸ
                'uid_arr': arr_display,  # åº”ç»­ARR
                'customer_classification': customer_classification,  # å®¢æˆ·åˆ†ç±»
                'sales': sales,  # ç»­è´¹è´£ä»»é”€å”®
                'sales_cn_en': sales_cn_en,  # è´£ä»»é”€å”®ä¸­è‹±æ–‡
                'jdy_sales': jdy_sales,  # ç®€é“äº‘é”€å”®
                'user_id': str(customer_data.get('ç”¨æˆ·ID', ''))  # ä¿ç•™ç”¨æˆ·IDç”¨äºå…¼å®¹
            })

        logger.info(f"æŸ¥è¯¢æˆåŠŸï¼Œæ‰¾åˆ°{len(results)}æ¡åŒ¹é…è®°å½•")
        return jsonify({'results': results})

    except Exception as e:
        logger.error(f"æŸ¥è¯¢å‡ºé”™: {str(e)}")
        return jsonify({'error': f'æŸ¥è¯¢å‡ºé”™: {str(e)}'}), 500

@app.route('/update_stage', methods=['POST'])
@login_required
def update_stage():
    """æ›´æ–°å®¢æˆ·é˜¶æ®µçŠ¶æ€ï¼ˆä½¿ç”¨ä¼˜åŒ–çš„çŠ¶æ€ç®¡ç†å™¨ï¼‰"""
    try:
        data = request.get_json()
        if not data or 'jdy_id' not in data or 'stage' not in data:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°', 'error_type': 'validation'}), 400
        
        jdy_id = data['jdy_id']
        stage = data['stage']
        force = data.get('force', False)  # æ˜¯å¦å¼ºåˆ¶æ›´æ–°
        
        logger.info(f"æ›´æ–°å®¢æˆ·é˜¶æ®µ: {jdy_id} -> {stage} (force={force})")
        
        # ä½¿ç”¨æ–°çš„çŠ¶æ€ç®¡ç†å™¨
        if stage_manager:
            result = stage_manager.update_stage(
                jdy_id=jdy_id,
                target_stage=stage,
                force=force,
                metadata={
                    'source': 'web_interface',
                    'user_agent': request.headers.get('User-Agent', ''),
                    'ip': request.remote_addr,
                    'timestamp': datetime.now().isoformat()
                }
            )
            
            # æ ¹æ®ç»“æœè¿”å›é€‚å½“çš„HTTPçŠ¶æ€ç 
            if result['success']:
                return jsonify(result), 200
            else:
                error_type = result.get('error_type', 'unknown')
                if error_type == 'validation':
                    return jsonify(result), 400
                elif error_type == 'customer_not_found':
                    return jsonify(result), 404
                elif error_type == 'conflict':
                    return jsonify(result), 409
                else:
                    return jsonify(result), 500
        else:
            # é™çº§åˆ°åŸæœ‰é€»è¾‘
            logger.warning("çŠ¶æ€ç®¡ç†å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘")
            return _legacy_update_stage(jdy_id, stage)
        
    except Exception as e:
        logger.error(f"æ›´æ–°é˜¶æ®µå¤±è´¥: {str(e)}")
        return jsonify({
            'success': False, 
            'error': str(e),
            'error_type': 'system_error'
        }), 500

def _legacy_update_stage(jdy_id, stage):
    """åŸæœ‰çš„çŠ¶æ€æ›´æ–°é€»è¾‘ï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
    try:
        # è¯»å–Excelæ–‡ä»¶
        excel_path = os.path.join(os.getcwd(), 'å…­å¤§æˆ˜åŒºç®€é“äº‘å®¢æˆ·.xlsx')
        if not os.path.exists(excel_path):
            logger.error(f"Excelæ–‡ä»¶ä¸å­˜åœ¨: {excel_path}")
            return jsonify({'success': False, 'error': 'Excelæ–‡ä»¶ä¸å­˜åœ¨', 'error_type': 'file_not_found'}), 500
        
        # è¯»å–Excelæ–‡ä»¶
        ensure_pandas_imported()
        df = pd.read_excel(excel_path)
        logger.info(f"æˆåŠŸè¯»å–Excelæ–‡ä»¶ï¼Œå…±{len(df)}è¡Œæ•°æ®")
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
        if 'ç”¨æˆ·ID' not in df.columns:
            logger.error("Excelæ–‡ä»¶ä¸­ç¼ºå°‘'ç”¨æˆ·ID'åˆ—")
            return jsonify({'success': False, 'error': 'Excelæ–‡ä»¶æ ¼å¼é”™è¯¯ï¼šç¼ºå°‘ç”¨æˆ·IDåˆ—', 'error_type': 'column_missing'}), 500
        
        # æŸ¥æ‰¾åŒ¹é…çš„å®¢æˆ·è®°å½•
        matching_rows = df[df['ç”¨æˆ·ID'].astype(str).str.contains(str(jdy_id), case=False, na=False)]
        
        if matching_rows.empty:
            logger.warning(f"æœªæ‰¾åˆ°åŒ¹é…çš„å®¢æˆ·è®°å½•: {jdy_id}")
            return jsonify({'success': False, 'error': f'æœªæ‰¾åˆ°å®¢æˆ·è®°å½•: {jdy_id}', 'error_type': 'customer_not_found'}), 404
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é˜¶æ®µåˆ—ï¼Œå¦‚æœæ²¡æœ‰åˆ™åˆ›å»º
        stage_column = 'å®¢æˆ·é˜¶æ®µ'
        if stage_column not in df.columns:
            df[stage_column] = ''
            logger.info(f"åˆ›å»ºæ–°åˆ—: {stage_column}")
        
        # æ›´æ–°åŒ¹é…è®°å½•çš„é˜¶æ®µ
        updated_count = 0
        for index in matching_rows.index:
            old_stage = df.loc[index, stage_column] if pd.notna(df.loc[index, stage_column]) else 'æœªè®¾ç½®'
            df.loc[index, stage_column] = stage
            updated_count += 1
            logger.info(f"æ›´æ–°è®°å½• {index}: {old_stage} -> {stage}")
        
        # ä¿å­˜æ›´æ–°åçš„Excelæ–‡ä»¶
        df.to_excel(excel_path, index=False)
        logger.info(f"Excelæ–‡ä»¶å·²æ›´æ–°ï¼Œå…±æ›´æ–° {updated_count} æ¡è®°å½•")
        
        return jsonify({
            'success': True,
            'message': f'å®¢æˆ· {jdy_id} å·²æˆåŠŸæ¨è¿›åˆ° {stage} é˜¶æ®µï¼ˆæ›´æ–°äº† {updated_count} æ¡è®°å½•ï¼‰',
            'updated_count': updated_count
        })
        
    except Exception as e:
         logger.error(f"Excelæ–‡ä»¶æ“ä½œå¤±è´¥: {str(e)}")
         return jsonify({'success': False, 'error': f'Excelæ–‡ä»¶æ“ä½œå¤±è´¥: {str(e)}', 'error_type': 'file_operation_error'}), 500

@app.route('/stage_history', methods=['GET'])
@login_required
def get_stage_history():
    """è·å–çŠ¶æ€å˜æ›´å†å²"""
    try:
        jdy_id = request.args.get('jdy_id')
        limit = int(request.args.get('limit', 100))
        
        if stage_manager:
            history = stage_manager.get_stage_history(jdy_id, limit)
            return jsonify({
                'success': True,
                'history': history,
                'count': len(history)
            })
        else:
            return jsonify({
                'success': False,
                'error': 'çŠ¶æ€ç®¡ç†å™¨ä¸å¯ç”¨',
                'error_type': 'service_unavailable'
            }), 503
            
    except Exception as e:
        logger.error(f"è·å–çŠ¶æ€å†å²å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e),
            'error_type': 'system_error'
        }), 500

@app.route('/validate_stage_batch', methods=['POST'])
@login_required
def validate_stage_batch():
    """æ‰¹é‡çŠ¶æ€æ ¡éªŒ"""
    try:
        data = request.get_json()
        if not data or 'updates' not in data:
            return jsonify({
                'success': False,
                'error': 'ç¼ºå°‘å¿…è¦å‚æ•°ï¼šupdates',
                'error_type': 'validation'
            }), 400
        
        updates = data['updates']
        if not isinstance(updates, list):
            return jsonify({
                'success': False,
                'error': 'updateså¿…é¡»æ˜¯æ•°ç»„',
                'error_type': 'validation'
            }), 400
        
        if stage_manager:
            results = stage_manager.validate_stage_batch(updates)
            return jsonify({
                'success': True,
                'results': results
            })
        else:
            return jsonify({
                'success': False,
                'error': 'çŠ¶æ€ç®¡ç†å™¨ä¸å¯ç”¨',
                'error_type': 'service_unavailable'
            }), 503
            
    except Exception as e:
        logger.error(f"æ‰¹é‡çŠ¶æ€æ ¡éªŒå¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e),
            'error_type': 'system_error'
        }), 500

@app.route('/stage_rules', methods=['GET'])
@login_required
def get_stage_rules():
    """è·å–çŠ¶æ€è½¬æ¢è§„åˆ™"""
    try:
        if stage_manager:
            return jsonify({
                'success': True,
                'stage_rules': stage_manager.stage_rules,
                'stage_priority': stage_manager.stage_priority
            })
        else:
            return jsonify({
                'success': False,
                'error': 'çŠ¶æ€ç®¡ç†å™¨ä¸å¯ç”¨',
                'error_type': 'service_unavailable'
            }), 503
            
    except Exception as e:
        logger.error(f"è·å–çŠ¶æ€è§„åˆ™å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e),
            'error_type': 'system_error'
        }), 500

@app.route('/docx_templates/<template_name>')
def get_template(template_name):
    if not template_name.endswith('.docx'):
        return 'ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹', 400

    try:
        # ä½¿ç”¨åº”ç”¨æ ¹è·¯å¾„å®šä½æ¨¡æ¿ç›®å½•ï¼Œé¿å…å·¥ä½œç›®å½•å·®å¼‚
        template_dir = os.path.join(app.root_path, 'templates', 'docx_templates')
        # é˜²æ­¢è·¯å¾„ç©¿è¶Šæ”»å‡»ï¼šè§„èŒƒåŒ–å¹¶æ£€æŸ¥å‰ç¼€
        candidate = os.path.normpath(os.path.join(template_dir, template_name))
        template_dir_abs = os.path.abspath(template_dir)
        if not candidate.startswith(template_dir_abs + os.sep):
            logger.error(f"éæ³•æ¨¡æ¿è·¯å¾„: {candidate}")
            return 'éæ³•æ¨¡æ¿è·¯å¾„', 400

        template_path = candidate

        if not os.path.exists(template_path):
            logger.error(f"æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {template_path}")
            return 'æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨', 404

        logger.info(f"æ­£åœ¨åŠ è½½æ¨¡æ¿æ–‡ä»¶: {template_path}")
        return send_file(
            template_path,
            as_attachment=False,
            mimetype='application/vnd.openxmlformats-officedocument.wordprocessingml.document'
        )
    except Exception as e:
        logger.error(f"æ¨¡æ¿æ–‡ä»¶è®¿é—®é”™è¯¯: {str(e)}")
        return 'æ¨¡æ¿æ–‡ä»¶è®¿é—®é”™è¯¯', 500

@app.route('/generate', methods=['POST'])
@login_required
def generate():
    temp_template = None
    temp_output = None
    
    try:
        logger.info("å¼€å§‹å¤„ç†æ–‡ä»¶ä¸Šä¼ è¯·æ±‚")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶ä¸Šä¼ 
        if 'template' not in request.files:
            logger.error("è¯·æ±‚ä¸­æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶")
            return 'è¯·é€‰æ‹©åˆåŒæ¨¡æ¿æ–‡ä»¶', 400
            
        template_file = request.files['template']
        if not template_file or not template_file.filename:
            logger.error("æ²¡æœ‰é€‰æ‹©æ–‡ä»¶")
            return 'è¯·é€‰æ‹©åˆåŒæ¨¡æ¿æ–‡ä»¶', 400
            
        if not template_file.filename.endswith('.docx'):
            logger.error("æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®")
            return 'è¯·é€‰æ‹©.docxæ ¼å¼çš„æ–‡ä»¶', 400

        logger.info(f"æ¥æ”¶åˆ°æ–‡ä»¶: {template_file.filename}")

        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜ä¸Šä¼ çš„æ¨¡æ¿
        temp_template = tempfile.NamedTemporaryFile(suffix='.docx', delete=False)
        template_file.save(temp_template.name)
        logger.info(f"æ¨¡æ¿æ–‡ä»¶å·²ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶: {temp_template.name}")
        
        # è·å–è¡¨å•æ•°æ®
        form_data = request.form.to_dict()
        
        # å¤„ç†å¤šé€‰å€¼
        contract_types = request.form.getlist('contract_type')
        if contract_types:
            form_data['contract_types'] = ', '.join(contract_types)
        
        logger.info(f"æ¥æ”¶åˆ°çš„è¡¨å•æ•°æ®: {form_data}")
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç”¨äºä¿å­˜ç”Ÿæˆçš„åˆåŒ
        temp_output = tempfile.NamedTemporaryFile(suffix='.docx', delete=False)
        logger.info(f"åˆ›å»ºè¾“å‡ºä¸´æ—¶æ–‡ä»¶: {temp_output.name}")

        # æ£€æŸ¥æ¨¡æ¿å¤„ç†å™¨æ˜¯å¦å¯ç”¨
        if not TEMPLATE_HANDLER_AVAILABLE:
            logger.error("æ¨¡æ¿å¤„ç†å™¨ä¸å¯ç”¨")
            return 'æ¨¡æ¿å¤„ç†å™¨ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ä¾èµ–åŒ…æ˜¯å¦æ­£ç¡®å®‰è£…', 500
            
        # å¤„ç†æ¨¡æ¿ï¼ˆæ‰©å¤§å¼‚å¸¸æ•è·èŒƒå›´ï¼Œè¦†ç›–åˆå§‹åŒ–ä¸æ¸²æŸ“é˜¶æ®µï¼‰
        try:
            handler = TemplateHandler(temp_template.name)
            output_path = handler.process_template(form_data, temp_output.name)
        except TemplateSyntaxError as e:
            # é’ˆå¯¹Jinjaæ¨¡æ¿è¯­æ³•é”™è¯¯è¿›è¡Œæ›´å‹å¥½çš„æç¤º
            logger.error(f"æ¨¡æ¿è¯­æ³•é”™è¯¯: {str(e)}")
            hint = (
                "æ¨¡æ¿è¯­æ³•é”™è¯¯ï¼ˆå¯èƒ½å­˜åœ¨å¤šä½™æˆ–ä¸åŒ¹é…çš„èŠ±æ‹¬å·ï¼‰ã€‚è¯·æ£€æŸ¥æ¨¡æ¿ä¸­çš„ Jinja å˜é‡/å—æ˜¯å¦æˆå¯¹ï¼š"
                "'{{ ... }}'ã€'{% ... %}'ã€'{# ... #}'ï¼›é¿å…å‡ºç°é¢å¤–çš„ '}'ã€‚å¦‚éœ€åœ¨æ­£æ–‡ä¸­ä½¿ç”¨èŠ±æ‹¬å·ï¼Œ"
                "è¯·æ”¹ç”¨å…¨è§’ 'ï½›ï½' æˆ–ä½¿ç”¨åŸå§‹å— '{% raw %}...{% endraw %}'."
            )
            # è¿”å›ç»“æ„åŒ–é”™è¯¯ï¼Œå‰ç«¯ä¼šä¼˜å…ˆè§£æJSONé”™è¯¯
            return jsonify({
                'error': f"ç”ŸæˆåˆåŒæ—¶å‘ç”Ÿé”™è¯¯: {getattr(e, 'message', str(e))}",
                'hint': hint,
                'lineno': getattr(e, 'lineno', None)
            }), 400
        except Exception as e:
            # docxtpl å¯èƒ½åŒ…è£… Jinja å¼‚å¸¸æˆ–ä»…è¿”å›é”™è¯¯ä¿¡æ¯
            msg = str(e)
            if (
                "unexpected '}'" in msg
                or 'jinja2' in msg.lower()
                or (DocxTplTemplateError and isinstance(e, DocxTplTemplateError))
            ):
                logger.error(f"æ¨¡æ¿è¯­æ³•é”™è¯¯(å…¼å®¹æŠ“å–): {msg}")
                hint = (
                    "æ¨¡æ¿è¯­æ³•é”™è¯¯ï¼ˆå¯èƒ½å­˜åœ¨å¤šä½™æˆ–ä¸åŒ¹é…çš„èŠ±æ‹¬å·ï¼‰ã€‚è¯·æ£€æŸ¥æ¨¡æ¿ä¸­çš„ Jinja å˜é‡/å—æ˜¯å¦æˆå¯¹ï¼š"
                    "'{{ ... }}'ã€'{% ... %}'ã€'{# ... #}'ï¼›é¿å…å‡ºç°é¢å¤–çš„ '}'ã€‚å¦‚éœ€åœ¨æ­£æ–‡ä¸­ä½¿ç”¨èŠ±æ‹¬å·ï¼Œ"
                    "è¯·æ”¹ç”¨å…¨è§’ 'ï½›ï½' æˆ–ä½¿ç”¨åŸå§‹å— '{% raw %}...{% endraw %}'."
                )
                return jsonify({
                    'error': f"ç”ŸæˆåˆåŒæ—¶å‘ç”Ÿé”™è¯¯: {msg}",
                    'hint': hint,
                    'lineno': getattr(e, 'lineno', None)
                }), 400
            # å…¶ä»–é”™è¯¯æŒ‰æœåŠ¡å™¨é”™è¯¯å¤„ç†
            logger.error(f"æ¨¡æ¿å¤„ç†å¤±è´¥: {msg}")
            return jsonify({'error': f'ç”ŸæˆåˆåŒæ—¶å‘ç”Ÿé”™è¯¯: {msg}'}), 500

        logger.info(f"åˆåŒç”Ÿæˆå®Œæˆï¼Œè¾“å‡ºæ–‡ä»¶: {output_path}")

        # è¿”å›ç”Ÿæˆçš„æ–‡ä»¶
        return send_file(
            output_path,
            as_attachment=True,
            download_name=f"{form_data['start_year']}-{form_data['end_year']}+{form_data['company_name']}+å¸†è½¯ç®€é“äº‘ç»­è´¹åˆåŒ+{datetime.now().strftime('%Y%m%d')}.docx",
            mimetype='application/vnd.openxmlformats-officedocument.wordprocessingml.document'
        )

    except Exception as e:
        logger.error(f"å‘ç”Ÿé”™è¯¯: {str(e)}")
        return jsonify({'error': f'ç”ŸæˆåˆåŒæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}'}), 500

    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            if temp_template:
                os.unlink(temp_template.name)
                logger.info(f"æ¸…ç†ä¸´æ—¶æ¨¡æ¿æ–‡ä»¶: {temp_template.name}")
            if temp_output:
                os.unlink(temp_output.name)
                logger.info(f"æ¸…ç†ä¸´æ—¶è¾“å‡ºæ–‡ä»¶: {temp_output.name}")
        except Exception as e:
            logger.error(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

@app.route('/generate_quote', methods=['POST'])
@login_required
def generate_quote():
    """ç”ŸæˆæŠ¥ä»·å•åŠŸèƒ½ - ç›´æ¥ä½¿ç”¨å›ºå®šæ¨¡æ¿"""
    temp_output = None
    
    try:
        logger.info("å¼€å§‹å¤„ç†æŠ¥ä»·å•ç”Ÿæˆè¯·æ±‚")
        
        # è·å–è¡¨å•æ•°æ®
        form_data = request.form.to_dict()
        logger.info(f"æ¥æ”¶åˆ°çš„è¡¨å•æ•°æ®: {form_data}")
        
        # éªŒè¯å¿…å¡«å­—æ®µ
        required_fields = ['company_name', 'tax_number', 'jdy_account', 'total_amount', 'user_count']
        for field in required_fields:
            if not form_data.get(field):
                logger.error(f"ç¼ºå°‘å¿…å¡«å­—æ®µ: {field}")
                return f'è¯·å¡«å†™{field}', 400
        
        # æ£€æŸ¥æ¨¡æ¿å¤„ç†å™¨æ˜¯å¦å¯ç”¨
        if not TEMPLATE_HANDLER_AVAILABLE:
            logger.error("æ¨¡æ¿å¤„ç†å™¨ä¸å¯ç”¨")
            return 'æ¨¡æ¿å¤„ç†å™¨ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ä¾èµ–åŒ…æ˜¯å¦æ­£ç¡®å®‰è£…', 500
        
        # ä½¿ç”¨å›ºå®šçš„æŠ¥ä»·å•æ¨¡æ¿
        quote_template_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'templates', 'docx_templates', 'ã€2025å¹´ç»­è´¹ã€‘ æŠ¥ä»·å•-å¸¦å˜é‡.doc')
        if not os.path.exists(quote_template_path):
            logger.error(f"æŠ¥ä»·å•æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {quote_template_path}")
            return 'æŠ¥ä»·å•æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨', 500
        
        logger.info(f"ä½¿ç”¨æŠ¥ä»·å•æ¨¡æ¿: {quote_template_path}")
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç”¨äºä¿å­˜ç”Ÿæˆçš„æŠ¥ä»·å•
        temp_output = tempfile.NamedTemporaryFile(suffix='.docx', delete=False)
        logger.info(f"åˆ›å»ºè¾“å‡ºä¸´æ—¶æ–‡ä»¶: {temp_output.name}")
        
        # å¤„ç†æ¨¡æ¿
        handler = TemplateHandler(quote_template_path)
        output_path = handler.process_template(form_data, temp_output.name)
        logger.info(f"æŠ¥ä»·å•ç”Ÿæˆå®Œæˆï¼Œè¾“å‡ºæ–‡ä»¶: {output_path}")

        # è¿”å›ç”Ÿæˆçš„æ–‡ä»¶
        return send_file(
            output_path,
            as_attachment=True,
            download_name=f"{form_data['company_name']}_æŠ¥ä»·å•_{datetime.now().strftime('%Y%m%d')}.docx",
            mimetype='application/vnd.openxmlformats-officedocument.wordprocessingml.document'
        )

    except Exception as e:
        logger.error(f"ç”ŸæˆæŠ¥ä»·å•æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
         return jsonify({'success': False, 'error': f'ç”ŸæˆæŠ¥ä»·å•æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}'}), 500
        
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            if temp_output:
                os.unlink(temp_output.name)
                logger.info(f"æ¸…ç†ä¸´æ—¶è¾“å‡ºæ–‡ä»¶: {temp_output.name}")
        except Exception as e:
            logger.error(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

@app.route('/parse_text', methods=['POST'])
@login_required
def parse_text():
    """è§£æç²˜è´´æ¿æ–‡æœ¬å†…å®¹"""
    try:
        data = request.get_json()
        if not data or 'text' not in data:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æä¾›æ–‡æœ¬å†…å®¹'}), 400
        
        text = data['text'].strip()
        if not text:
            return jsonify({'success': False, 'error': 'æ–‡æœ¬å†…å®¹ä¸ºç©º'}), 400
        
        logger.info(f"å¼€å§‹è§£ææ–‡æœ¬ï¼Œé•¿åº¦: {len(text)} å­—ç¬¦")
        logger.info(f"å®é™…æ–‡æœ¬å†…å®¹: {repr(text)}")  # æ·»åŠ è¯¦ç»†æ—¥å¿—
        
        # ä½¿ç”¨OCRæœåŠ¡çš„æ–‡æœ¬è§£æåŠŸèƒ½
        if ocr_service:
            parsed_fields = ocr_service.parse_text_to_fields(text)
            
            # æ£€æµ‹O/0æ··æ·†è­¦å‘Š
            warnings = []
            if 'tax_number' in parsed_fields:
                tax_number = parsed_fields['tax_number']
                # æ£€æµ‹ç¨å·ä¸­æ˜¯å¦åŒ…å«å­—æ¯Oæˆ–æ•°å­—0ï¼ˆä»»æ„ä¸€ä¸ªéƒ½æé†’ï¼‰
                if 'O' in tax_number or '0' in tax_number:
                    warnings.append({
                        'type': 'ocr_confusion',
                        'field': 'tax_number',
                        'message': 'ç¨å·åŒ…å«0/O,è¯·æ³¨æ„æ£€æŸ¥ğŸ§',
                        'suggestion': ''
                    })
            
            logger.info(f"æ–‡æœ¬è§£ææˆåŠŸï¼Œè¯†åˆ«åˆ° {len(parsed_fields)} ä¸ªå­—æ®µï¼Œ{len(warnings)} ä¸ªè­¦å‘Š")
            return jsonify({
                'success': True,
                'fields': parsed_fields,
                'field_count': len(parsed_fields),
                'warnings': warnings
            })
        else:
            # å¦‚æœOCRæœåŠ¡ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•çš„æ–‡æœ¬è§£æ
            parsed_fields = simple_text_parse(text)
            logger.info(f"ç®€å•æ–‡æœ¬è§£æå®Œæˆï¼Œè¯†åˆ«åˆ° {len(parsed_fields)} ä¸ªå­—æ®µ")
            return jsonify({
                'success': True,
                'fields': parsed_fields,
                'field_count': len(parsed_fields)
            })
    
    except Exception as e:
        logger.error(f"æ–‡æœ¬è§£æå¼‚å¸¸: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'æ–‡æœ¬è§£æå¤±è´¥: {str(e)}',
            'fields': {},
            'field_count': 0
        }), 500

@app.route('/ocr_image', methods=['POST'])
@login_required
def ocr_image():
    """å¤„ç†OCRå›¾ç‰‡è¯†åˆ«è¯·æ±‚ï¼ˆbase64æ ¼å¼ï¼‰"""
    try:
        data = request.get_json()
        if not data or 'image' not in data:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æä¾›å›¾ç‰‡æ•°æ®'}), 400
        
        image_data = data['image']
        if not image_data:
            return jsonify({'success': False, 'error': 'å›¾ç‰‡æ•°æ®ä¸ºç©º'}), 400
        
        # å¤„ç†base64å›¾ç‰‡æ•°æ®
        if image_data.startswith('data:image'):
            # ç§»é™¤data:image/xxx;base64,å‰ç¼€
            image_data = image_data.split(',')[1]
        
        try:
            import base64
            image_bytes = base64.b64decode(image_data)
        except Exception as e:
            return jsonify({'success': False, 'error': 'å›¾ç‰‡æ•°æ®æ ¼å¼é”™è¯¯'}), 400
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆé™åˆ¶ä¸º10MBï¼‰
        if len(image_bytes) > 10 * 1024 * 1024:
            return jsonify({'success': False, 'error': 'å›¾ç‰‡å¤§å°è¶…è¿‡é™åˆ¶ï¼ˆ10MBï¼‰'}), 400
        
        logger.info(f"å¼€å§‹å¤„ç†OCRè¯·æ±‚ï¼Œå›¾ç‰‡å¤§å°: {len(image_bytes)} bytes")
        
        # ä½¿ç”¨OCRæœåŠ¡å¤„ç†å›¾ç‰‡
        if ocr_service:
            # ä½¿ç”¨process_imageæ–¹æ³•è€Œä¸æ˜¯extract_text_from_image
            result = ocr_service.process_image(image_bytes)
            if result['success']:
                # æ£€æµ‹O/0æ··æ·†è­¦å‘Š
                warnings = []
                parsed_fields = result.get('parsed_fields', {})
                if 'tax_number' in parsed_fields:
                    tax_number = parsed_fields['tax_number']
                    # æ£€æµ‹ç¨å·ä¸­æ˜¯å¦åŒ…å«å­—æ¯Oæˆ–æ•°å­—0ï¼ˆä»»æ„ä¸€ä¸ªéƒ½æé†’ï¼‰
                    if 'O' in tax_number or '0' in tax_number:
                        warnings.append({
                            'type': 'ocr_confusion',
                            'field': 'tax_number',
                            'message': 'ç¨å·åŒ…å«0/O,è¯·æ³¨æ„æ£€æŸ¥ğŸ§',
                            'suggestion': ''
                        })
                
                logger.info(f"OCRå¤„ç†æˆåŠŸï¼Œæå–æ–‡æœ¬é•¿åº¦: {len(result.get('extracted_text', ''))}")
                return jsonify({
                    'success': True,
                    'text': result.get('extracted_text', ''),
                    'confidence': 0.8,  # é»˜è®¤ç½®ä¿¡åº¦
                    'fields': result.get('parsed_fields', {}),
                    'field_count': result.get('field_count', 0),
                    'warnings': warnings
                })
            else:
                logger.error(f"OCRå¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                return jsonify({
                    'success': False,
                    'error': result.get('error', 'OCRè¯†åˆ«å¤±è´¥'),
                    'text': ''
                })
        else:
            return jsonify({
                'success': False,
                'error': 'OCRæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ä½¿ç”¨ç²˜è´´æ¿åŠŸèƒ½',
                'text': ''
            })
    
    except Exception as e:
        logger.error(f"OCRå›¾ç‰‡å¤„ç†å¼‚å¸¸: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'å›¾ç‰‡å¤„ç†å¤±è´¥: {str(e)}',
            'text': ''
        }), 500

def simple_text_parse(text):
    """ç®€å•çš„æ–‡æœ¬è§£æåŠŸèƒ½ï¼Œå½“OCRæœåŠ¡ä¸å¯ç”¨æ—¶ä½¿ç”¨"""
    import re
    
    fields = {}
    
    # å®šä¹‰å­—æ®µåŒ¹é…è§„åˆ™
    patterns = {
        'company_name': [
            r'å…¬å¸åç§°[ï¼š:]\s*(.+?)(?:\n|$)',
            r'ä¼ä¸šåç§°[ï¼š:]\s*(.+?)(?:\n|$)',
            r'å\s*ç§°[ï¼š:]\s*(.+?)(?:\n|$)',
            r'å•ä½åç§°[ï¼š:]\s*(.+?)(?:\n|$)'
        ],
        'tax_number': [
            r'ç¨å·[ï¼š:]\s*([A-Z0-9]{15,20})(?:\n|$)',
            r'çº³ç¨äººè¯†åˆ«å·[ï¼š:]\s*([A-Z0-9]{15,20})(?:\n|$)',
            r'ç»Ÿä¸€ç¤¾ä¼šä¿¡ç”¨ä»£ç [ï¼š:]\s*([A-Z0-9]{15,20})(?:\n|$)'
        ],
        'reg_address': [
            r'æ³¨å†Œåœ°å€[ï¼š:]\s*(.+?)(?:\n|$)',
            r'åœ°\s*å€[ï¼š:]\s*(.+?)(?:\n|$)',
            r'æ³¨å†Œåœ°[ï¼š:]\s*(.+?)(?:\n|$)'
        ],
        'reg_phone': [
            r'æ³¨å†Œç”µè¯[ï¼š:]\s*([0-9\-\s]+)(?:\n|$)',
            r'ç”µ\s*è¯[ï¼š:]\s*([0-9\-\s]+)(?:\n|$)',
            r'è”ç³»ç”µè¯[ï¼š:]\s*([0-9\-\s]+)(?:\n|$)'
        ],
        'bank_name': [
            r'å¼€æˆ·è¡Œ[ï¼š:]\s*(.+?)(?:\n|$)',
            r'å¼€æˆ·é“¶è¡Œ[ï¼š:]\s*(.+?)(?:\n|$)',
            r'é“¶è¡Œåç§°[ï¼š:]\s*(.+?)(?:\n|$)'
        ],
        'bank_account': [
            r'è´¦å·[ï¼š:]\s*([0-9\s]+)(?:\n|$)',
            r'é“¶è¡Œè´¦å·[ï¼š:]\s*([0-9\s]+)(?:\n|$)',
            r'è´¦æˆ·[ï¼š:]\s*([0-9\s]+)(?:\n|$)'
        ],
        'contact_name': [
            r'è”ç³»äºº[ï¼š:]\s*(.+?)(?:\n|$)',
            r'è´Ÿè´£äºº[ï¼š:]\s*(.+?)(?:\n|$)',
            r'ç»åŠäºº[ï¼š:]\s*(.+?)(?:\n|$)'
        ],
        'contact_phone': [
            r'è”ç³»äººç”µè¯[ï¼š:]\s*([0-9\-\s]+)(?:\n|$)',
            r'æ‰‹æœº[ï¼š:]\s*([0-9\-\s]+)(?:\n|$)',
            r'ç§»åŠ¨ç”µè¯[ï¼š:]\s*([0-9\-\s]+)(?:\n|$)'
        ],
        'mail_address': [
            r'é‚®å¯„åœ°å€[ï¼š:]\s*(.+?)(?:\n|$)',
            r'é€šè®¯åœ°å€[ï¼š:]\s*(.+?)(?:\n|$)',
            r'æ”¶ä»¶åœ°å€[ï¼š:]\s*(.+?)(?:\n|$)'
        ]
    }
    
    # å¯¹æ¯ä¸ªå­—æ®µè¿›è¡ŒåŒ¹é…
    for field_name, field_patterns in patterns.items():
        for pattern in field_patterns:
            match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE)
            if match:
                value = match.group(1).strip()
                if value and len(value) > 0:
                    fields[field_name] = value
                    break
    
    return fields

@app.route('/ocr_process', methods=['POST'])
def ocr_process():
    """å¤„ç†OCRå›¾ç‰‡è¯†åˆ«è¯·æ±‚"""
    try:
        # æ£€æŸ¥OCRæœåŠ¡æ˜¯å¦å¯ç”¨
        if ocr_service is None:
            return jsonify({
                'success': False,
                'error': 'OCRæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•'
            }), 503

        # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶ä¸Šä¼ 
        if 'image' not in request.files:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰ä¸Šä¼ å›¾ç‰‡æ–‡ä»¶'}), 400

        file = request.files['image']
        if file.filename == '':
            return jsonify({'success': False, 'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'}), 400

        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        allowed_extensions = {'png', 'jpg', 'jpeg', 'gif', 'bmp', 'tiff', 'webp'}
        file_ext = file.filename.rsplit('.', 1)[1].lower() if '.' in file.filename else ''

        if file_ext not in allowed_extensions:
            return jsonify({'success': False, 'error': 'ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼'}), 400

        # è¯»å–å›¾ç‰‡æ•°æ®
        image_data = file.read()

        # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆé™åˆ¶ä¸º10MBï¼‰
        if len(image_data) > 10 * 1024 * 1024:
            return jsonify({'success': False, 'error': 'æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶ï¼ˆ10MBï¼‰'}), 400

        logger.info(f"å¼€å§‹å¤„ç†OCRè¯·æ±‚ï¼Œæ–‡ä»¶å¤§å°: {len(image_data)} bytes")

        # ä½¿ç”¨OCRæœåŠ¡å¤„ç†å›¾ç‰‡
        result = ocr_service.process_image(image_data)

        if result['success']:
            logger.info(f"OCRå¤„ç†æˆåŠŸï¼Œè¯†åˆ«åˆ° {result['field_count']} ä¸ªå­—æ®µ")
            return jsonify(result)
        else:
            logger.error(f"OCRå¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            # å¯¹äºOCRä¸å¯ç”¨çš„æƒ…å†µï¼Œè¿”å›200çŠ¶æ€ç ä½†success=False
            # è¿™æ ·å‰ç«¯å¯ä»¥æ­£ç¡®å¤„ç†é”™è¯¯ä¿¡æ¯
            return jsonify(result), 200

    except Exception as e:
        logger.error(f"OCRå¤„ç†å¼‚å¸¸: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'æœåŠ¡å™¨å¤„ç†é”™è¯¯: {str(e)}',
            'extracted_text': '',
            'parsed_fields': {},
            'field_count': 0
        }), 500

@app.route('/monitor_downloads', methods=['POST'])
@login_required
def monitor_downloads():
    """ç›‘æ§ä¸‹è½½æ–‡ä»¶å¤¹ä¸­çš„åˆåŒæ–‡ä»¶å¹¶è‡ªåŠ¨æ›´æ–°å®¢æˆ·çŠ¶æ€"""
    try:
        import os
        from pathlib import Path
        import time
        import re
        
        # è·å–ç”¨æˆ·çš„ä¸‹è½½æ–‡ä»¶å¤¹è·¯å¾„
        downloads_path = str(Path.home() / "Downloads")
        
        # æ£€æŸ¥ä¸‹è½½æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
        if not os.path.exists(downloads_path):
            return jsonify({
                'success': False, 
                'error': 'æ— æ³•è®¿é—®ä¸‹è½½æ–‡ä»¶å¤¹',
                'path': downloads_path
            }), 400
        
        # è·å–æœ€è¿‘30åˆ†é’Ÿå†…çš„åˆåŒæ–‡ä»¶
        current_time = time.time()
        recent_contracts = []
        
        try:
            for file_path in Path(downloads_path).glob("*.docx"):
                # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´ï¼ˆæœ€è¿‘30åˆ†é’Ÿå†…ï¼‰
                file_mtime = file_path.stat().st_mtime
                if current_time - file_mtime <= 1800:  # 30åˆ†é’Ÿ = 1800ç§’
                    # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«åˆåŒç›¸å…³å…³é”®è¯
                    filename = file_path.name
                    if any(keyword in filename for keyword in ['åˆåŒ', 'ç»­è´¹', 'å¸†è½¯', 'ç®€é“äº‘']):
                        # å°è¯•ä»æ–‡ä»¶åä¸­æå–ç®€é“äº‘è´¦å·
                        jdy_account = extract_jdy_account_from_filename(filename)
                        recent_contracts.append({
                            'filename': filename,
                            'path': str(file_path),
                            'mtime': file_mtime,
                            'jdy_account': jdy_account
                        })
        except Exception as e:
            logger.error(f"æ‰«æä¸‹è½½æ–‡ä»¶å¤¹å¤±è´¥: {str(e)}")
            return jsonify({
                'success': False,
                'error': f'æ‰«ææ–‡ä»¶å¤¹å¤±è´¥: {str(e)}'
            }), 500
        
        # è‡ªåŠ¨æ¨è¿›æ‰¾åˆ°ç®€é“äº‘è´¦å·çš„åˆåŒåˆ°"åˆåŒ"é˜¶æ®µ
        updated_contracts = []
        for contract in recent_contracts:
            if contract['jdy_account']:
                try:
                    # è°ƒç”¨æ›´æ–°é˜¶æ®µçš„é€»è¾‘
                    result = update_customer_stage(contract['jdy_account'], 'åˆåŒ')
                    if result['success']:
                        updated_contracts.append({
                            'filename': contract['filename'],
                            'jdy_account': contract['jdy_account'],
                            'status': 'updated'
                        })
                        # è®°å½•å·²å¤„ç†çš„æ–‡ä»¶
                        processed_files[contract['path']] = current_time
                        logger.info(f"è‡ªåŠ¨æ¨è¿›åˆåŒæˆåŠŸ: {contract['jdy_account']} -> åˆåŒé˜¶æ®µ")
                    else:
                        updated_contracts.append({
                            'filename': contract['filename'],
                            'jdy_account': contract['jdy_account'],
                            'status': 'failed',
                            'error': result.get('error', 'æœªçŸ¥é”™è¯¯')
                        })
                        # å³ä½¿å¤±è´¥ä¹Ÿè®°å½•ï¼Œé¿å…é‡å¤å°è¯•
                        processed_files[contract['path']] = current_time
                except Exception as e:
                    logger.error(f"è‡ªåŠ¨æ¨è¿›åˆåŒå¤±è´¥: {str(e)}")
                    updated_contracts.append({
                        'filename': contract['filename'],
                        'jdy_account': contract['jdy_account'],
                        'status': 'error',
                        'error': str(e)
                    })
                    # å³ä½¿å‡ºé”™ä¹Ÿè®°å½•ï¼Œé¿å…é‡å¤å°è¯•
                    processed_files[contract['path']] = current_time
            else:
                # æ²¡æœ‰æ‰¾åˆ°ç®€é“äº‘è´¦å·çš„æ–‡ä»¶ä¹Ÿè®°å½•ï¼Œé¿å…é‡å¤æ‰«æ
                processed_files[contract['path']] = current_time
        
        return jsonify({
            'success': True,
            'downloads_path': downloads_path,
            'recent_contracts': recent_contracts,
            'updated_contracts': updated_contracts,
            'total_found': len(recent_contracts),
            'total_updated': len([c for c in updated_contracts if c['status'] == 'updated'])
        })
        
    except Exception as e:
        logger.error(f"ç›‘æ§ä¸‹è½½æ–‡ä»¶å¤¹å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'ç›‘æ§åŠŸèƒ½å¼‚å¸¸: {str(e)}'
        }), 500

def extract_jdy_account_from_filename(filename):
    """ä»æ–‡ä»¶åä¸­æå–ç®€é“äº‘è´¦å·"""
    import re
    
    # å°è¯•å¤šç§æ¨¡å¼åŒ¹é…ç®€é“äº‘è´¦å·
    patterns = [
        r'([a-f0-9]{24,})',  # 24ä½ä»¥ä¸Šçš„åå…­è¿›åˆ¶å­—ç¬¦ä¸²
        r'([a-zA-Z0-9]{20,})',  # 20ä½ä»¥ä¸Šçš„å­—æ¯æ•°å­—ç»„åˆ
        r'jdy[_-]?([a-zA-Z0-9]+)',  # jdyå‰ç¼€
        r'è´¦å·[_-]?([a-zA-Z0-9]+)',  # è´¦å·å‰ç¼€
    ]
    
    for pattern in patterns:
        match = re.search(pattern, filename, re.IGNORECASE)
        if match:
            account = match.group(1)
            # éªŒè¯è´¦å·é•¿åº¦å’Œæ ¼å¼
            if len(account) >= 15 and len(account) <= 50:
                return account
    
    return None

def update_customer_stage(jdy_id, stage):
    """æ›´æ–°å®¢æˆ·é˜¶æ®µçš„å†…éƒ¨å‡½æ•°"""
    try:
        # è¯»å–Excelæ–‡ä»¶
        excel_path = os.path.join(os.getcwd(), 'å…­å¤§æˆ˜åŒºç®€é“äº‘å®¢æˆ·.xlsx')
        if not os.path.exists(excel_path):
            return {'success': False, 'error': 'Excelæ–‡ä»¶ä¸å­˜åœ¨'}
        
        ensure_pandas_imported()
        df = pd.read_excel(excel_path)
        
        # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
        if 'ç”¨æˆ·ID' not in df.columns:
            return {'success': False, 'error': 'Excelæ–‡ä»¶æ ¼å¼é”™è¯¯ï¼šç¼ºå°‘ç”¨æˆ·IDåˆ—'}
        
        # æŸ¥æ‰¾åŒ¹é…çš„å®¢æˆ·è®°å½•
        matching_rows = df[df['ç”¨æˆ·ID'].astype(str).str.contains(str(jdy_id), case=False, na=False)]
        
        if matching_rows.empty:
            return {'success': False, 'error': f'æœªæ‰¾åˆ°å®¢æˆ·è®°å½•: {jdy_id}'}
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é˜¶æ®µåˆ—ï¼Œå¦‚æœæ²¡æœ‰åˆ™åˆ›å»º
        stage_column = 'å®¢æˆ·é˜¶æ®µ'
        if stage_column not in df.columns:
            df[stage_column] = ''
        
        # æ›´æ–°åŒ¹é…è®°å½•çš„é˜¶æ®µ
        updated_count = 0
        for index in matching_rows.index:
            df.loc[index, stage_column] = stage
            updated_count += 1
        
        # ä¿å­˜æ›´æ–°åçš„Excelæ–‡ä»¶
        df.to_excel(excel_path, index=False)
        
        return {
            'success': True,
            'message': f'å®¢æˆ· {jdy_id} å·²æˆåŠŸæ¨è¿›åˆ° {stage} é˜¶æ®µ',
            'updated_count': updated_count
        }
        
    except Exception as e:
        return {'success': False, 'error': f'æ›´æ–°å¤±è´¥: {str(e)}'}

def background_monitor_worker():
    """åå°ç›‘æ§å·¥ä½œçº¿ç¨‹"""
    global auto_monitor_enabled, monitor_results, last_monitor_check
    
    while auto_monitor_enabled:
        try:
            with monitor_lock:
                if not auto_monitor_enabled:
                    break
                    
                # æ‰§è¡Œç›‘æ§æ£€æŸ¥
                result = perform_monitor_check()
                if result:
                    monitor_results = result
                    last_monitor_check = datetime.now()
                    
                    # å¦‚æœå‘ç°æ–°åˆåŒï¼Œè®°å½•æ—¥å¿—
                    if result.get('total_found', 0) > 0:
                        logger.info(f"è‡ªåŠ¨ç›‘æ§å‘ç° {result['total_found']} ä¸ªåˆåŒæ–‡ä»¶ï¼ŒæˆåŠŸæ¨è¿› {result.get('total_updated', 0)} ä¸ª")
                        
        except Exception as e:
            logger.error(f"åå°ç›‘æ§å¼‚å¸¸: {str(e)}")
            
        # ç­‰å¾…30ç§’åç»§ç»­ä¸‹ä¸€æ¬¡æ£€æŸ¥
        time.sleep(30)
    
    logger.info("åå°ç›‘æ§çº¿ç¨‹å·²åœæ­¢")

def perform_monitor_check():
    """æ‰§è¡Œç›‘æ§æ£€æŸ¥çš„æ ¸å¿ƒé€»è¾‘"""
    global processed_files
    try:
        # è·å–ç”¨æˆ·çš„ä¸‹è½½æ–‡ä»¶å¤¹è·¯å¾„
        downloads_path = str(Path.home() / "Downloads")
        
        # æ£€æŸ¥ä¸‹è½½æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
        if not os.path.exists(downloads_path):
            return None
        
        # è·å–æœ€è¿‘30åˆ†é’Ÿå†…çš„åˆåŒæ–‡ä»¶
        current_time = time.time()
        recent_contracts = []
        
        # æ¸…ç†è¶…è¿‡24å°æ—¶çš„å·²å¤„ç†æ–‡ä»¶è®°å½•
        files_to_remove = []
        for file_path, process_time in processed_files.items():
            if current_time - process_time > 86400:  # 24å°æ—¶ = 86400ç§’
                files_to_remove.append(file_path)
        for file_path in files_to_remove:
            del processed_files[file_path]
        
        for file_path in Path(downloads_path).glob("*.docx"):
            # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´ï¼ˆæœ€è¿‘30åˆ†é’Ÿå†…ï¼‰
            file_mtime = file_path.stat().st_mtime
            if current_time - file_mtime <= 1800:  # 30åˆ†é’Ÿ = 1800ç§’
                # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«åˆåŒç›¸å…³å…³é”®è¯
                filename = file_path.name
                if any(keyword in filename for keyword in ['åˆåŒ', 'ç»­è´¹', 'å¸†è½¯', 'ç®€é“äº‘']):
                    # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡è¿™ä¸ªæ–‡ä»¶
                    file_path_str = str(file_path)
                    if file_path_str in processed_files:
                        continue  # è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶
                    
                    # å°è¯•ä»æ–‡ä»¶åä¸­æå–ç®€é“äº‘è´¦å·
                    jdy_account = extract_jdy_account_from_filename(filename)
                    recent_contracts.append({
                        'filename': filename,
                        'path': file_path_str,
                        'mtime': file_mtime,
                        'jdy_account': jdy_account
                    })
        
        # è‡ªåŠ¨æ¨è¿›æ‰¾åˆ°ç®€é“äº‘è´¦å·çš„åˆåŒåˆ°"åˆåŒ"é˜¶æ®µ
        updated_contracts = []
        for contract in recent_contracts:
            # å¤„ç†å®Œæˆåç«‹å³è®°å½•æ–‡ä»¶ï¼Œé¿å…é‡å¤å¤„ç†
            processed_files[contract['path']] = current_time
            
            if contract['jdy_account']:
                try:
                    # è°ƒç”¨æ›´æ–°é˜¶æ®µçš„é€»è¾‘
                    result = update_customer_stage(contract['jdy_account'], 'åˆåŒ')
                    if result['success']:
                        updated_contracts.append({
                            'filename': contract['filename'],
                            'jdy_account': contract['jdy_account'],
                            'status': 'updated'
                        })
                        logger.info(f"è‡ªåŠ¨æ¨è¿›åˆåŒæˆåŠŸ: {contract['jdy_account']} -> åˆåŒé˜¶æ®µ")
                    else:
                        updated_contracts.append({
                            'filename': contract['filename'],
                            'jdy_account': contract['jdy_account'],
                            'status': 'failed',
                            'error': result.get('error', 'æœªçŸ¥é”™è¯¯')
                        })
                except Exception as e:
                    logger.error(f"è‡ªåŠ¨æ¨è¿›åˆåŒå¤±è´¥: {str(e)}")
                    updated_contracts.append({
                        'filename': contract['filename'],
                        'jdy_account': contract['jdy_account'],
                        'status': 'error',
                        'error': str(e)
                    })
            else:
                # æ²¡æœ‰æ‰¾åˆ°ç®€é“äº‘è´¦å·çš„æ–‡ä»¶ä¹Ÿè®°å½•ï¼Œé¿å…é‡å¤æ‰«æ
                logger.debug(f"æ–‡ä»¶ {contract['filename']} æœªæ‰¾åˆ°ç®€é“äº‘è´¦å·ï¼Œå·²è®°å½•è·³è¿‡")
        
        return {
            'downloads_path': downloads_path,
            'recent_contracts': recent_contracts,
            'updated_contracts': updated_contracts,
            'total_found': len(recent_contracts),
            'total_updated': len([c for c in updated_contracts if c['status'] == 'updated']),
            'last_check': datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"ç›‘æ§æ£€æŸ¥å¤±è´¥: {str(e)}")
        return None

@app.route('/start_auto_monitor', methods=['POST'])
@login_required
def start_auto_monitor():
    """å¯åŠ¨è‡ªåŠ¨ç›‘æ§åŠŸèƒ½"""
    global auto_monitor_enabled, monitor_thread
    
    try:
        with monitor_lock:
            if auto_monitor_enabled:
                return jsonify({
                    'success': False,
                    'error': 'è‡ªåŠ¨ç›‘æ§å·²åœ¨è¿è¡Œä¸­'
                }), 400
            
            # å¯åŠ¨åå°ç›‘æ§
            auto_monitor_enabled = True
            monitor_thread = threading.Thread(target=background_monitor_worker, daemon=True)
            monitor_thread.start()
            
            logger.info("åå°è‡ªåŠ¨ç›‘æ§å·²å¯åŠ¨")
            
            return jsonify({
                'success': True,
                'message': 'åå°è‡ªåŠ¨ç›‘æ§å·²å¯åŠ¨',
                'status': 'running'
            })
        
    except Exception as e:
        logger.error(f"å¯åŠ¨è‡ªåŠ¨ç›‘æ§å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'å¯åŠ¨ç›‘æ§å¤±è´¥: {str(e)}'
        }), 500

@app.route('/stop_auto_monitor', methods=['POST'])
@login_required
def stop_auto_monitor():
    """åœæ­¢è‡ªåŠ¨ç›‘æ§åŠŸèƒ½"""
    global auto_monitor_enabled, monitor_thread, processed_files
    
    try:
        with monitor_lock:
            if not auto_monitor_enabled:
                return jsonify({
                    'success': False,
                    'error': 'è‡ªåŠ¨ç›‘æ§æœªåœ¨è¿è¡Œ'
                }), 400
            
            # åœæ­¢åå°ç›‘æ§
            auto_monitor_enabled = False
            
            # ç­‰å¾…çº¿ç¨‹ç»“æŸ
            if monitor_thread and monitor_thread.is_alive():
                monitor_thread.join(timeout=5)
            
            monitor_thread = None
            
            # æ¸…ç†å·²å¤„ç†æ–‡ä»¶è®°å½•
            processed_files.clear()
            
            logger.info("åå°è‡ªåŠ¨ç›‘æ§å·²åœæ­¢ï¼Œå·²æ¸…ç†å¤„ç†è®°å½•")
            
            return jsonify({
                'success': True,
                'message': 'åå°è‡ªåŠ¨ç›‘æ§å·²åœæ­¢',
                'status': 'stopped'
            })
        
    except Exception as e:
        logger.error(f"åœæ­¢è‡ªåŠ¨ç›‘æ§å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'åœæ­¢ç›‘æ§å¤±è´¥: {str(e)}'
        }), 500

@app.route('/get_monitor_status', methods=['GET'])
@login_required
def get_monitor_status():
    """è·å–ç›‘æ§çŠ¶æ€å’Œç»“æœ"""
    global auto_monitor_enabled, monitor_results, last_monitor_check
    
    try:
        with monitor_lock:
            return jsonify({
                'success': True,
                'enabled': auto_monitor_enabled,
                'last_check': last_monitor_check.isoformat() if last_monitor_check else None,
                'results': monitor_results
            })
        
    except Exception as e:
        logger.error(f"è·å–ç›‘æ§çŠ¶æ€å¤±è´¥: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'è·å–çŠ¶æ€å¤±è´¥: {str(e)}'
        }), 500


def auto_start_monitor():
    """åº”ç”¨å¯åŠ¨æ—¶è‡ªåŠ¨å¯åŠ¨ç›‘æ§åŠŸèƒ½"""
    global auto_monitor_enabled, monitor_thread
    
    try:
        with monitor_lock:
            if not auto_monitor_enabled:
                # å¯åŠ¨åå°ç›‘æ§
                auto_monitor_enabled = True
                monitor_thread = threading.Thread(target=background_monitor_worker, daemon=True)
                monitor_thread.start()
                logger.info("åº”ç”¨å¯åŠ¨æ—¶è‡ªåŠ¨å¯åŠ¨åå°ç›‘æ§æˆåŠŸ")
            else:
                logger.info("åå°ç›‘æ§å·²åœ¨è¿è¡Œä¸­")
    except Exception as e:
        logger.error(f"è‡ªåŠ¨å¯åŠ¨ç›‘æ§å¤±è´¥: {str(e)}")

if __name__ == '__main__':
    # ç¯å¢ƒé…ç½®
    port = int(os.environ.get('PORT', 8080))
    # ç”Ÿäº§ç¯å¢ƒä½¿ç”¨0.0.0.0ï¼Œå¼€å‘ç¯å¢ƒä½¿ç”¨localhost
    if os.environ.get('FLASK_ENV') == 'production':
        host = '0.0.0.0'
    else:
        host = os.environ.get('HOST', 'localhost')
    debug = os.environ.get('FLASK_ENV') != 'production'
    
    print(f"æ­£åœ¨å¯åŠ¨Flaskåº”ç”¨...")
    print(f"ç«¯å£: {port}")
    print(f"ä¸»æœº: {host}")
    print(f"è°ƒè¯•æ¨¡å¼: {debug}")
    print(f"ç¯å¢ƒ: {os.environ.get('FLASK_ENV', 'development')}")
    
    # åº”ç”¨å¯åŠ¨æ—¶è‡ªåŠ¨å¯åŠ¨ç›‘æ§ - ä¸´æ—¶ç¦ç”¨ç”¨äºè°ƒè¯•
    # try:
    #     auto_start_monitor()
    #     print("è‡ªåŠ¨ç›‘æ§å¯åŠ¨å®Œæˆ")
    # except Exception as e:
    #     print(f"è‡ªåŠ¨ç›‘æ§å¯åŠ¨å¤±è´¥: {str(e)}")
    print("è‡ªåŠ¨ç›‘æ§å·²ä¸´æ—¶ç¦ç”¨ç”¨äºè°ƒè¯•")

    print("å¼€å§‹å¯åŠ¨FlaskæœåŠ¡å™¨...")
    app.run(debug=debug, port=port, host=host)
